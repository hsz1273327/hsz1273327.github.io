---
layout: post
title: "基于MI50的AIPC探索"
series:
    aipc_experiment:
        index: 4
date: 2025-10-09
author: "Hsz"
category: experiment
tags:
    - Linux
    - AIPC
    - Rocm
    - LLM
header-img: "img/home-bg-o.jpg"
update: 2025-10-20
---
# 基于MI50的APIC探索

MI50这张卡是一个优缺点都很明显的卡.它的优点是显存大,带宽高,缺点是算力不行,功耗也不低,而且它的定位是计算卡,因此没有显示输出接口,只能用作计算用途.本文主要介绍我基于MI50的AIPC探索.

## AMD MI50的定位

MI50有16g和32g两个版本,16g版本一般在500以内,32g版本在800以内.首先它是雷7的计算卡版本,但驱动阉割了视频输出功能,也阉割了风扇供电和pwm功能.买到手我们需要先给它配一个散热才能正常使用,否则负载一上来就会因为过热而降频甚至黑屏死机.

通常买这个卡的都是冲着便宜,一般有两个定位

+ 16g版本很多垃圾佬买回来刷雷7的vbios用来打游戏的.这种用法还需要额外配一根能主动输出视频信号的minidp线,成本也就百十块,再加上散热,总成本也不会高于800,足以应付绝大多数1080p分辨率下的游戏需求,还是很值的.不过我们不讨论这种用法.
+ 32g版本则是冲着大显存和高带宽用来跑大模型的.32g版本的显存带宽高达1024GB/s,足以应付目前绝大多数大模型的推理需求.而且32g版本的价格也不高,对于预算有限但又想跑大模型的用户来说是一个不错的选择.本文显然是讨论这种用法.

## 散热改造

这卡买回来是没有主动散热的,我们需要给它配一个散热模块.目前市面上主流的散热方案有3种:

+ 水冷散热: 这种方案最为激进,但也最为复杂.需要配一个水冷头,水泵,水箱,散热排等.冷排什么的用二手的成本也就百元内,散热效果最好,可以最大化发挥这张卡的性能,甚至你还能超频用,也安静,但安装难度最大,需要较强的动手能力,而且由于要暴力拆解肯定会丧失店保(虽然店保可以认为没保),适合追求极致性能且有一定动手能力的用户.

+ 风扇风冷散热: 这种方案是最为常见的,也是最为简单的,主要依靠3d打印件替换原本的外壳,至于,造型也很多,有双槽单40系显卡风扇的,有双槽双40系显卡风扇的,也有增加偏斜角度到2.5槽用9025甚至9030单风扇的.这些方案都只是换个壳,不需要动到显卡本体,因此也不会丧失店保.但散热效果就比较差强人意了,毕竟被动散热的鳍片设计并不是为这种风冷方案设计的,而且风道也不理想,因此散热效果一般.但好处是安装简单,成本低,适合大部分用户.配个热传感器监控温度,在低负载下风扇转速也不会太高,噪音也能接受.

+ 涡轮风冷散热: 这种方案是最为折中的,由于风道更加匹配,它的散热效果介于水冷和风扇风冷之间,安装一般也是换个壳,不需要动到显卡本体,因此也不会丧失店保.但缺点是噪音较大而且听感不好.这种方案适合对噪音不敏感但又想要较好散热效果的用户.我使用的就是这种方案.

两种风冷方案的风扇启停功能可以有两种方案:

1. 可以配一个热传感器用于监控显卡温度以调节风扇转速.风扇的供电其实mi50上有,如果你有动手能力也可以接个端子上去,如果不想动手,也可以用usb供电的风扇或者用sata供电转,这样就不需要动手了.我使用的就是从显卡上接电这种方案,主要是我希望这张卡的使用体验可以尽量和普通显卡接近,这样如果以后放在显卡坞中使用可以不用考虑额外的接线.

2. 直接用主板上的风扇4pin,这样就可以直接用主板的风扇控制功能来调节风扇转速.但缺点是风扇启停功能依赖于主板和软件,如果主板不支持监控显卡温度,就只能设置成风扇一直转,要跑任务时手动拉高转速,不跑任务时手动拉低转速.

## MI50 32g的能力范围

这张卡属于老一代的vega20架构, 虽然有rocm支持,但由于架构老旧,很多新特性并不支持比如(bf16,fp8,fp4,int4等).我们可以简单的总结下它的能力范围:

+ 跑llama.cpp的模型,如果想折腾rocm可以用ollama的rocm后端,不想折腾可以用lmstudio的vulkan后端.vulkan后端似乎在一些模型中表现比rocm还好些.

+ 用pytorch跑comfyui的放大任务,可以使用fp16尺寸的模型或gguf模型,充分利用fp16算力和大显存和高带宽优势,但由于算力不行,因此直接生图的速度尴尬,但用来跑放大刚好可以规避算力不足的短板.

+ 如果有多张卡可以用<https://github.com/nlzy/vllm-gfx906>项目提供的镜像跑vllm充分利用张量并行技术,也可以尝试使用<https://github.com/InternLM/lmdeploy>项目,它现在支持rocm

不要对它有过多不切实际的期望,它的算力太低,而且没有cuda生态,只能跑推理,而且大多数加速方案都不支持.

## rocm环境搭建

todo

## llm性能测试和本地模型推荐

我们以相同的模型,相同的量化等级和相同的测试方法对mi50上不同的推理框架进行测试


> 稠密模型

我们选择`gemma3:27b-it-qat`作为稠密模型的测试对象,并设置上下文长度为`128k`.测试方法是使用相同的prompt进行推理,并且测量推理速度和显存占用.

+ 识图任务

prompt:

```txt

```

| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (ROCm from ollama)     | ---                       | ---          | ---        |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |


+ 看图说故事任务

prompt:

```txt

```

| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (ROCm from ollama)     | ---                       | ---          | ---        |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |


+ 初中几何题作答

prompt:

```txt

```

| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (ROCm from ollama)     | ---                       | ---          | ---        |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |

+ 图片文字提取任务

prompt:

```txt

```

| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (ROCm from ollama)     | ---                       | ---          | ---        |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |

> moe模型

+ 初中代数题作答

prompt:

```txt

```

我们选择`qwen3:30b-a3b-thinking-2507-q4_K_M`作为moe模型的测试对象,并设置上下文长度为`128k`.测试方法是使用相同的prompt进行推理,并且测量推理速度和显存占用.

| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (ROCm from ollama)     | ---                       | ---          | ---        |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |

+ 高考命题作文

prompt:

```txt

```

我们选择`qwen3:30b-a3b-thinking-2507-q4_K_M`作为moe模型的测试对象,并设置上下文长度为`128k`.测试方法是使用相同的prompt进行推理,并且测量推理速度和显存占用.

| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (ROCm from ollama)     | ---                       | ---          | ---        |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |

+ 简单前端编程题

prompt:

```txt

```

我们选择`qwen3:30b-a3b-thinking-2507-q4_K_M`作为moe模型的测试对象,并设置上下文长度为`128k`.测试方法是使用相同的prompt进行推理,并且测量推理速度和显存占用.

| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (ROCm from ollama)     | ---                       | ---          | ---        |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |

+ 任务执行计划定制

prompt:

```txt

```

我们选择`qwen3:30b-a3b-thinking-2507-q4_K_M`作为moe模型的测试对象,并设置上下文长度为`128k`.测试方法是使用相同的prompt进行推理,并且测量推理速度和显存占用.

| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (ROCm from ollama)     | ---                       | ---          | ---        |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |
