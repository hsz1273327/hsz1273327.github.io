---
layout: post
title: "基于MI50的AIPC探索"
series:
    aipc_experiment:
        index: 4
date: 2025-10-18
author: "Hsz"
category: experiment
tags:
    - Linux
    - AIPC
    - Rocm
    - LLM
header-img: "img/home-bg-o.jpg"
update: 2025-10-20
---
# 基于MI50的APIC探索

MI50这张卡是一个优缺点都很明显的卡.它的优点是显存大,带宽高,缺点是算力不行,功耗也不低,而且它的定位是计算卡,因此没有显示输出接口,只能用作计算用途.本文主要介绍我基于MI50的AIPC探索.

## AMD MI50的定位

MI50有16g和32g两个版本,16g版本一般在500以内,32g版本在800以内.首先它是雷7的计算卡版本,但驱动阉割了视频输出功能,也阉割了风扇供电和pwm功能.买到手我们需要先给它配一个散热才能正常使用,否则负载一上来就会因为过热而降频甚至黑屏死机.

通常买这个卡的都是冲着便宜,一般有两个定位

+ 16g版本很多垃圾佬买回来刷雷7的vbios用来打游戏的.这种用法还需要额外配一根能主动输出视频信号的minidp线,成本也就百十块,再加上散热,总成本也不会高于800,足以应付绝大多数1080p分辨率下的游戏需求,还是很值的.不过我们不讨论这种用法.
+ 32g版本则是冲着大显存和高带宽用来跑大模型的.32g版本的显存带宽高达1024GB/s,足以应付目前绝大多数大模型的推理需求.而且32g版本的价格也不高,对于预算有限但又想跑大模型的用户来说是一个不错的选择.本文显然是讨论这种用法.

## 散热改造

这卡买回来是没有主动散热的,我们需要给它配一个散热模块.目前市面上主流的散热方案有3种:

+ 水冷散热: 这种方案最为激进,但也最为复杂.需要配一个水冷头,水泵,水箱,散热排等.冷排什么的用二手的成本也就百元内,散热效果最好,可以最大化发挥这张卡的性能,甚至你还能超频用,也安静,但安装难度最大,需要较强的动手能力,而且由于要暴力拆解肯定会丧失店保(虽然店保可以认为没保),适合追求极致性能且有一定动手能力的用户.

+ 风扇风冷散热: 这种方案是最为常见的,也是最为简单的,主要依靠3d打印件替换原本的外壳,至于,造型也很多,有双槽单40系显卡风扇的,有双槽双40系显卡风扇的,也有增加偏斜角度到2.5槽用9025甚至9030单风扇的.这些方案都只是换个壳,不需要动到显卡本体,因此也不会丧失店保.但散热效果就比较差强人意了,毕竟被动散热的鳍片设计并不是为这种风冷方案设计的,而且风道也不理想,因此散热效果一般.但好处是安装简单,成本低,适合大部分用户.配个热传感器监控温度,在低负载下风扇转速也不会太高,噪音也能接受.

+ 涡轮风冷散热: 这种方案是最为折中的,由于风道更加匹配,它的散热效果介于水冷和风扇风冷之间,安装一般也是换个壳,不需要动到显卡本体,因此也不会丧失店保.但缺点是噪音较大而且听感不好.这种方案适合对噪音不敏感但又想要较好散热效果的用户.我使用的就是这种方案.

两种风冷方案的风扇启停功能可以有两种方案:

1. 可以配一个热传感器用于监控显卡温度以调节风扇转速.风扇的供电其实mi50上有,如果你有动手能力也可以接个端子上去,如果不想动手,也可以用usb供电的风扇或者用sata供电转,这样就不需要动手了.我使用的就是从显卡上接电这种方案,主要是我希望这张卡的使用体验可以尽量和普通显卡接近,这样如果以后放在显卡坞中使用可以不用考虑额外的接线.

2. 直接用主板上的风扇4pin,这样就可以直接用主板的风扇控制功能来调节风扇转速.但缺点是风扇启停功能依赖于主板和软件,如果主板不支持监控显卡温度,就只能设置成风扇一直转,要跑任务时手动拉高转速,不跑任务时手动拉低转速.

## MI50 32g的能力范围

这张卡属于老一代的vega20架构, 虽然有rocm支持,但由于架构老旧,很多新特性并不支持比如(bf16,fp8,fp4,int4等).我们可以简单的总结下它的能力范围:

+ 跑llama.cpp的模型,如果想折腾rocm可以用ollama的rocm后端,不想折腾可以用lmstudio的vulkan后端.vulkan后端似乎在一些模型中表现比rocm还好些.

+ 用pytorch跑comfyui的放大任务,可以使用fp16尺寸的模型或gguf模型,充分利用fp16算力和大显存和高带宽优势,但由于算力不行,因此直接生图的速度尴尬,但用来跑放大刚好可以规避算力不足的短板.

+ 如果有多张卡可以用<https://github.com/nlzy/vllm-gfx906>项目提供的镜像跑vllm充分利用张量并行技术,也可以尝试使用<https://github.com/InternLM/lmdeploy>项目,它现在支持rocm

不要对它有过多不切实际的期望,它的算力太低,而且没有cuda生态,只能跑推理,而且大多数加速方案都不支持.

## rocm环境搭建

截止到2025年10月,rocm的最新版本是7.0.2,MI50被官方支持的最后一个版本是6.3.3.我们用的是ubuntu,而6.3.3版本的rocm官方预编译的包只支持到ubuntu 24.04lts的6.8和6.11内核版本.但ubuntu 24.04lts随着时间的变化现在已经切换到了使用6.14内核,因此现在官方的rocm 6.3.3包已经无法在ubuntu 24.04lts上使用了.现在要给mi50装rocm环境有两个方案:

1. 退回到使用ubuntu 22.04.5lts,这个版本已经不再更新,使用的是6.8内核,可以直接安装rocm 6.3.3官方包,但缺点是系统版本过旧,软件包版本也过旧.
2. 继续使用ubuntu 24.04lts,使用6.4.3版本的rocm,由于这个版本rocm源码中MI50的部分并没有删,只是官方没有给出预编译包,而这版rocm是支持6.14内核的,因此我们可以先安装预编译版本,用自己编译MI50相关的部分给它打补丁.我使用的就是这种方案.

### 为MI 50安装rocm 6.4.3

幸运的是代MI50支持的rocm 6.4.3已经[被Arch社区打包好了](https://archlinux.org/packages/extra/x86_64/rocblas/)我们可以手动用它为环境中的rocm打补丁从而使用可以正常使用rocm 6.4.3的效果.

1. 按照官方文档安装rocm 6.4.3版本,安装完成后重启系统.这个和给a卡安驱动差不多

    ```bash
    wget https://repo.radeon.com/amdgpu-install/6.4.3/ubuntu/noble/amdgpu-install_6.4.60403-1_all.deb
    sudo apt install ./amdgpu-install_6.4.60403-1_all.deb
    sudo apt update
    sudo apt install python3-setuptools python3-wheel
    sudo usermod -a -G render,video $LOGNAME # Add the current user to the render and video groups
    sudo apt install rocm
    ```

    之后重启后记得将环境变量加入到`~/.bashrc`中

    ```bash
    echo 'export ROCM_PATH=/opt/rocm-6.4.3' >> ~/.bashrc
    ```

2. 去下载被Arch社区打包好的[rocm 6.4.3的rocblas包](https://archlinux.org/packages/extra/x86_64/rocblas/download/),解压

    ```bash

    ```

3. 解压得到的文件夹中有源码有编译好的二进制文件,我们在``中搜索`fx906`相关的文件,选中它们然后考出来到一个单独的文件夹中备用(比如叫``).

4. 进入到rocm的安装目录下的`/opt/rocm/lib`目录中,把刚才考出来的文件复制进去即可.

5. 如果你后面要安装rocm版本的pytorch,还需要在安装好pytorch后把这些文件复制到`<你的python环境>/site-packages/torch/lib/`目录中.

## MI50 的其他相关环境

这里汇总下我在MI50上跑AIPC相关的环境和版本

### pytorch相关生态

rocm的pytorch生态建议不要使用conda构造的python环境,而是使用系统python环境构造虚拟环境来搭建,因为rocm相关的库是系统级安装的,它会依赖系统的编译环境,用conda的独立环境会报错.

```bash
python3 -m venv ~/pyvenv/mi50-venv
source ~/pyvenv/mi50-venv/bin/activate
```

在激活了该虚拟环境的情况下我们就可以安装pytorch和相关生态了.我使用的是pytorch 2.9版本,可以直接用pip安装

```bash
pip install torch torchvision torchaudio  --index-url https://download.pytorch.org/whl/rocm6.4
```

> 验证安装

我们可以使用pytorch官方的样例项目来验证安装是否成功

```bash
git clone https://github.com/pytorch/examples.git
cd examples/mnist
python main.py
```

如果能正常跑完且没有报错,且使用监控软件可以观察到MI50的显存和算力有被使用,说明安装成功.

> triton

大神nlzy[提供了一版专门针对MI50修改的triton](https://github.com/nlzy/triton-gfx906)我们可以直接用它来安装.

```bash
git clone https://github.com/nlzy/triton-gfx906.git
cd triton-gfx906
pip install ninja 'cmake<4' wheel pybind11
pip install --no-build-isolation .
```

注意最后一步会在后台下载大量第三方依赖进行编译,确保你的系统有足够的编译环境(建议48g内存以上)和对外网访问稳定的网络环境(挂好梯子),同时这个编译会持续较长时间,不要看着它没动就以为卡住了,耐心等它完成即可.

> vllm

vllm是一个高性能的llm推理框架,它主要的卖点是支持张量并行和流水线并行,可以充分利用多卡的算力来提升推理速度和并发性能.还是大神nlzy,它提供了一个[专门针对MI50修改的vllm版本](https://github.com/nlzy/vllm-gfx906).

>> Docker镜像使用
我们如果只是要用`vllm server`跑提供llm的调用api可以直接用docker镜像[nalanzeyu/vllm-gfx906](https://hub.docker.com/r/nalanzeyu/vllm-gfx906)来跑.

```bash
docker pull nalanzeyu/vllm-gfx906:latest
```

注意这个镜像超过5G,建议在有稳定网络的环境下拉取.如果是在docker的镜像站拉取,建议先加上在前面hostname尝试能不能拉动,因为很多镜像站的白名单中会屏蔽这种超大的镜像.

```bash
docker pull 镜像站的hostname/nalanzeyu/vllm-gfx906:latest
```

在拉取好后我们可以用下面的命令来跑vllm server

```bash
docker run -it --rm --shm-size=2g --device=/dev/kfd --device=/dev/dri \
    --group-add video -p 8000:8000 -v <YOUR_MODEL_PATH>:/model \
    nalanzeyu/vllm-gfx906 vllm serve /model --served-model-name <MODEL_NAME>
```

>> 本地编译使用

如果要在本地环境中在python环境下调用,可以参考它的README进行编译安装.但由于这个版本是针对的rocm 6.3.3的,我们需要修改`requirements/rocm-build.txt`中的依赖.我们不妨复制下来然后把里面的rocm版本号改成6.4.3即可.

+ `requirements/rocm-build643.txt`

    ```txt
    # Common dependencies
    -r common.txt

    --extra-index-url https://download.pytorch.org/whl/rocm6.4
    torch==2.9.0
    torchvision==0.24.0
    torchaudio==2.9.0

    triton==3.4.0
    cmake>=3.26.1,<4
    packaging>=24.2
    setuptools>=77.0.3,<80.0.0
    setuptools-scm>=8
    wheel
    jinja2>=3.1.6
    amdsmi>=6.2.4,<6.4
    timm>=1.0.17
    ```

之后安装依赖并编译:

```bash
pip3 install -r requirements/rocm-build643.txt
pip3 install -r requirements/rocm.txt

pip3 install --no-build-isolation .
```

> lmdeploy



这样就能使用了.

### llama.cpp生态

目前测试下来ollama的rocm后端镜像无法正常调用MI50,但lmstudio的vulkan后端可以正常使用,反正目前看评测vulkan后端性能全面超过rocm后端,因此推荐使用lmstudio的vulkan后端.


## llm性能测试和本地模型推荐

我们以相同的模型,相同的量化等级和相同的测试方法对mi50上不同的推理框架进行测试


> 稠密模型

我们选择`gemma3:27b-it-qat`作为稠密模型的测试对象,并设置上下文长度为`128k`.测试方法是使用相同的prompt进行推理,并且测量推理速度和显存占用.

+ 识图任务

prompt:

```txt

```

| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |


+ 看图说故事任务

prompt:

```txt

```

| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |


+ 初中几何题作答

prompt:

```txt

```

| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |

+ 图片文字提取任务

prompt:

```txt

```

| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |

> moe模型

+ 初中代数题作答

prompt:

```txt

```

我们选择`qwen3:30b-a3b-thinking-2507-q4_K_M`作为moe模型的测试对象,并设置上下文长度为`128k`.测试方法是使用相同的prompt进行推理,并且测量推理速度和显存占用.

| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |

+ 高考命题作文

prompt:

```txt

```

我们选择`qwen3:30b-a3b-thinking-2507-q4_K_M`作为moe模型的测试对象,并设置上下文长度为`128k`.测试方法是使用相同的prompt进行推理,并且测量推理速度和显存占用.

| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |

+ 简单前端编程题

prompt:

```txt

```

我们选择`qwen3:30b-a3b-thinking-2507-q4_K_M`作为moe模型的测试对象,并设置上下文长度为`128k`.测试方法是使用相同的prompt进行推理,并且测量推理速度和显存占用.

| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |

+ 任务执行计划定制

prompt:

```txt

```

我们选择`qwen3:30b-a3b-thinking-2507-q4_K_M`作为moe模型的测试对象,并设置上下文长度为`128k`.测试方法是使用相同的prompt进行推理,并且测量推理速度和显存占用.

| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |
