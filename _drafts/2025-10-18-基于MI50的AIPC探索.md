---
layout: post
title: "基于MI50的AIPC探索"
series:
    aipc_experiment:
        index: 4
date: 2025-10-18
author: "Hsz"
category: experiment
tags:
    - Linux
    - AIPC
    - Rocm
    - LLM
header-img: "img/home-bg-o.jpg"
update: 2025-10-20
---
# 基于MI50的APIC探索

MI50这张卡是一个优缺点都很明显的卡.它的优点是显存大,带宽高,缺点是算力不行,功耗也不低,而且它的定位是计算卡,因此没有显示输出接口,只能用作计算用途.本文主要介绍我基于MI50的AIPC探索.

## AMD MI50的定位

MI50有16g和32g两个版本,16g版本一般在500以内,32g版本在800以内.首先它是雷7的计算卡版本,但驱动阉割了视频输出功能,也阉割了风扇供电和pwm功能.买到手我们需要先给它配一个散热才能正常使用,否则负载一上来就会因为过热而降频甚至黑屏死机.

通常买这个卡的都是冲着便宜,一般有两个定位

+ 16g版本很多垃圾佬买回来刷雷7的vbios用来打游戏的.这种用法还需要额外配一根能主动输出视频信号的minidp线,成本也就百十块,再加上散热,总成本也不会高于800,足以应付绝大多数1080p分辨率下的游戏需求,还是很值的.不过我们不讨论这种用法.
+ 32g版本则是冲着大显存和高带宽用来跑大模型的.32g版本的显存带宽高达1024GB/s,足以应付目前绝大多数大模型的推理需求.而且32g版本的价格也不高,对于预算有限但又想跑大模型的用户来说是一个不错的选择.本文显然是讨论这种用法.

## 散热改造

这卡买回来是没有主动散热的,我们需要给它配一个散热模块.目前市面上主流的散热方案有3种:

+ 水冷散热: 这种方案最为激进,但也最为复杂.需要配一个水冷头,水泵,水箱,散热排等.冷排什么的用二手的成本也就百元内,散热效果最好,可以最大化发挥这张卡的性能,甚至你还能超频用,也安静,但安装难度最大,需要较强的动手能力,而且由于要暴力拆解肯定会丧失店保(虽然店保可以认为没保),适合追求极致性能且有一定动手能力的用户.

+ 风扇风冷散热: 这种方案是最为常见的,也是最为简单的,主要依靠3d打印件替换原本的外壳,至于,造型也很多,有双槽单40系显卡风扇的,有双槽双40系显卡风扇的,也有增加偏斜角度到2.5槽用9025甚至9030单风扇的.这些方案都只是换个壳,不需要动到显卡本体,因此也不会丧失店保.但散热效果就比较差强人意了,毕竟被动散热的鳍片设计并不是为这种风冷方案设计的,而且风道也不理想,因此散热效果一般.但好处是安装简单,成本低,适合大部分用户.配个热传感器监控温度,在低负载下风扇转速也不会太高,噪音也能接受.

+ 涡轮风冷散热: 这种方案是最为折中的,由于风道更加匹配,它的散热效果介于水冷和风扇风冷之间,安装一般也是换个壳,不需要动到显卡本体,因此也不会丧失店保.但缺点是噪音较大而且听感不好.这种方案适合对噪音不敏感但又想要较好散热效果的用户.我使用的就是这种方案.

两种风冷方案的风扇启停功能可以有两种方案:

1. 可以配一个热传感器用于监控显卡温度以调节风扇转速.风扇的供电其实mi50上有,如果你有动手能力也可以接个端子上去,如果不想动手,也可以用usb供电的风扇或者用sata供电转,这样就不需要动手了.我使用的就是从显卡上接电这种方案,主要是我希望这张卡的使用体验可以尽量和普通显卡接近,这样如果以后放在显卡坞中使用可以不用考虑额外的接线.

2. 直接用主板上的风扇4pin,这样就可以直接用主板的风扇控制功能来调节风扇转速.但缺点是风扇启停功能依赖于主板和软件,如果主板不支持监控显卡温度,就只能设置成风扇一直转,要跑任务时手动拉高转速,不跑任务时手动拉低转速.

## MI50 32g的能力范围

这张卡属于老一代的vega20架构, 虽然有rocm支持,但由于架构老旧,很多新特性并不支持比如(bf16,fp8,fp4,int4等).我们可以简单的总结下它的能力范围:

+ 跑llama.cpp的模型,如果想折腾rocm可以用ollama的rocm后端,不想折腾可以用lmstudio的vulkan后端.vulkan后端似乎在一些模型中表现比rocm还好些.

+ 用pytorch跑comfyui的放大任务,可以使用fp16尺寸的模型或gguf模型,充分利用fp16算力和大显存和高带宽优势,但由于算力不行,因此直接生图的速度尴尬,但用来跑放大刚好可以规避算力不足的短板.

+ 如果有多张卡可以用<https://github.com/nlzy/vllm-gfx906>项目提供的镜像跑vllm充分利用张量并行技术,也可以尝试使用<https://github.com/InternLM/lmdeploy>项目,它现在支持rocm

不要对它有过多不切实际的期望,它的算力太低,而且没有cuda生态,只能跑推理,而且大多数加速方案都不支持.

## rocm环境搭建

截止到2025年10月,rocm的最新版本是7.0.2,MI50被官方支持的最后一个版本是6.3.3.我们用的是ubuntu,而6.3.3版本的rocm官方预编译的包只支持到ubuntu 24.04lts的6.8和6.11内核版本.但ubuntu 24.04lts随着时间的变化现在已经切换到了使用6.14内核,因此现在官方的rocm 6.3.3包已经无法在ubuntu 24.04lts上使用了.现在要给mi50装rocm环境有两个方案:

1. 退回到使用ubuntu 22.04.5lts,这个版本已经不再更新,使用的是6.8内核,可以直接安装rocm 6.3.3官方包,但缺点是系统版本过旧,软件包版本也过旧.
2. 继续使用ubuntu 24.04lts,使用6.4.3版本的rocm,由于这个版本rocm源码中MI50的部分并没有删,只是官方没有给出预编译包,而这版rocm是支持6.14内核的,因此我们可以先安装预编译版本,用自己编译MI50相关的部分给它打补丁.我使用的就是这种方案.

### 为MI 50安装rocm 6.4.3

幸运的是代MI50支持的rocm 6.4.3已经[被Arch社区打包好了](https://archlinux.org/packages/extra/x86_64/rocblas/)我们可以手动用它为环境中的rocm打补丁从而使用可以正常使用rocm 6.4.3的效果.

1. 按照官方文档安装rocm 6.4.3版本,安装完成后重启系统.这个和给a卡安驱动差不多

    ```bash
    wget https://repo.radeon.com/amdgpu-install/6.4.3/ubuntu/noble/amdgpu-install_6.4.60403-1_all.deb
    sudo apt install ./amdgpu-install_6.4.60403-1_all.deb
    sudo apt update
    sudo apt install python3-setuptools python3-wheel
    sudo usermod -a -G render,video $LOGNAME # Add the current user to the render and video groups
    sudo apt install rocm
    ```

    之后重启后记得将环境变量加入到`~/.bashrc`中

    ```bash
    echo 'export ROCM_PATH=/opt/rocm-6.4.3' >> ~/.bashrc
    ```

2. 去下载被Arch社区打包好的[rocm 6.4.3的rocblas包](https://archlinux.org/packages/extra/x86_64/rocblas/download/),解压

    ```bash

    ```

3. 解压得到的文件夹中有源码有编译好的二进制文件,我们在``中搜索`fx906`相关的文件,选中它们然后考出来到一个单独的文件夹中备用(比如叫`rocmlibgfx906`).

4. 进入到rocm的安装目录下的`/opt/rocm/lib/rocblas/library`目录中,把刚才考出来的文件复制进去即可.

5. 如果你后面要安装rocm版本的pytorch,还需要在安装好pytorch后把这些文件复制到`<你的python环境>/site-packages/torch/lib/`目录中.

## MI50 的其他相关环境

这里汇总下我在MI50上跑AIPC相关的环境和版本

### pytorch相关生态

rocm的pytorch生态建议不要使用conda构造的python环境,而是使用系统python环境构造虚拟环境来搭建,因为rocm相关的库是系统级安装的,它会依赖系统的编译环境,用conda的独立环境会报错.

```bash
python3 -m venv ~/pyvenv/mi50-venv
source ~/pyvenv/mi50-venv/bin/activate
```

在激活了该虚拟环境的情况下我们就可以安装pytorch和相关生态了.我使用的是pytorch 2.9版本,可以直接用pip安装

```bash
pip install torch torchvision torchaudio  --index-url https://download.pytorch.org/whl/rocm6.4
```

> 验证安装

我们可以使用pytorch官方的样例项目来验证安装是否成功

```bash
git clone https://github.com/pytorch/examples.git
cd examples/mnist
python main.py
```

如果能正常跑完且没有报错,且使用监控软件可以观察到MI50的显存和算力有被使用,说明安装成功.

> triton

大神nlzy[提供了一版专门针对MI50修改的triton](https://github.com/nlzy/triton-gfx906)我们可以直接用它来安装.

```bash
git clone https://github.com/nlzy/triton-gfx906.git
cd triton-gfx906
pip install ninja 'cmake<4' wheel pybind11
pip install --no-build-isolation .
```

注意最后一步会在后台下载大量第三方依赖进行编译,确保你的系统有足够的编译环境(建议48g内存以上)和对外网访问稳定的网络环境(挂好梯子),同时这个编译会持续较长时间,不要看着它没动就以为卡住了,耐心等它完成即可.

> vllm

vllm是一个高性能的llm推理框架,它主要的卖点是支持张量并行和流水线并行,可以充分利用多卡的算力来提升推理速度和并发性能.还是大神nlzy,它提供了一个[专门针对MI50修改的vllm版本](https://github.com/nlzy/vllm-gfx906).

>> Docker镜像使用
我们如果只是要用`vllm server`跑提供llm的调用api可以直接用docker镜像[nalanzeyu/vllm-gfx906](https://hub.docker.com/r/nalanzeyu/vllm-gfx906)来跑.

```bash
docker pull nalanzeyu/vllm-gfx906:latest
```

注意这个镜像超过5G,建议在有稳定网络的环境下拉取.如果是在docker的镜像站拉取,建议先加上在前面hostname尝试能不能拉动,因为很多镜像站的白名单中会屏蔽这种超大的镜像.

```bash
docker pull 镜像站的hostname/nalanzeyu/vllm-gfx906:latest
```

在拉取好后我们可以用下面的命令来跑vllm server

```bash
docker run -it --rm --shm-size=2g --device=/dev/kfd --device=/dev/dri \
    --group-add video -p 8000:8000 -v <YOUR_MODEL_PATH>:/model \
    nalanzeyu/vllm-gfx906 vllm serve /model --served-model-name <MODEL_NAME>
```

>> 本地编译使用

如果要在本地环境中在python环境下调用,可以参考它的README进行编译安装.但由于这个版本是针对的rocm 6.3.3的,我们需要修改`requirements/rocm-build.txt`中的依赖.我们不妨复制下来然后把里面的rocm版本号改成6.4.3即可.

+ `requirements/rocm-build643.txt`

    ```txt
    # Common dependencies
    -r common.txt

    --extra-index-url https://download.pytorch.org/whl/rocm6.4
    torch==2.9.0
    torchvision==0.24.0
    torchaudio==2.9.0

    triton==3.4.0
    cmake>=3.26.1,<4
    packaging>=24.2
    setuptools>=77.0.3,<80.0.0
    setuptools-scm>=8
    wheel
    jinja2>=3.1.6
    amdsmi>=6.2.4,<6.4
    timm>=1.0.17
    ```

之后安装依赖并编译:

```bash
pip3 install -r requirements/rocm-build643.txt
pip3 install -r requirements/rocm.txt

pip3 install --no-build-isolation .
```

<!-- > lmdeploy
 -->

这样就能使用了.

### llama.cpp生态

目前测试下来ollama的rocm后端镜像无法正常调用MI50,但lmstudio的vulkan后端可以正常使用,反正目前看评测vulkan后端性能全面超过rocm后端,因此推荐使用lmstudio的vulkan后端.

## llm性能测试和本地模型推荐

在推荐模型前,我们先看看市面上的主流llm模型类型.

以输入内容为区分依据,我们可以把llm模型分为2大类:

+ 纯文本模型,这类模型是最常见的,只能识别文本,输出文本,比如经典的deepseek R1系列模型.
+ 多模态模型,这类模型可以识别图片和文本,并且可以输出文本,比如谷歌的gemma 3

他们在推理时主要区别在输入上下文的构建上,多模态模型需要把图片编码成向量然后和文本拼接在一起作为上下文输入,而纯文本模型则直接把文本作为上下文输入.每张图片编码成向量的大小一般在1k到4k之间,因此多模态模型需要的上下文长度会比纯文本模型更长一些.也就是说给prefill阶段带来的压力会更大.

根据输出是否会带有深度思考内容,我们可以把llm模型分为3大类:

+ 直答模型,这类模型主要用于直接回答用户的问题,输出简洁明了的答案,比如qwen3:4b-instruct.
+ 思考模型,这类模型主要用于需要深度思考和推理的任务,输出内容会包含思考过程和推理步骤,比如qwen3:30b-a3b-thinking.
+ embedding模型,这类模型主要用于生成文本的向量表示,以便进行相似度计算和检索,比如gme-Qwen2-VL-7B-Instruct

他们在推理时主要区别在输出上下文的构建上,思考模型需要在输出中包含思考过程和推理步骤,因此输出的上下文长度会更长.也就是说给decode阶段带来的压力会更大.同时由于输出的内容多,模型的应答会更慢,因此用户体验会差些.而embedding模型则不需要生成长文本,而是直接生成固定长度的向量,因此decode阶段的压力最小

以模型架构为区分依据,我们可以把llm模型分为2大类:

+ 稠密模型,这类模型是最常见的,所有参数都参与计算,比如qwen3-7b-instruct.
+ moe模型,这类模型是近年来兴起的,通过引入专家网络和路由机制来提升模型的表达能力和计算效率,比如qwen3:30b-a3b-thinking.

他们在推理时主要区别在计算资源的使用上,moe模型需要全量加载但并不需要全量计算,因此在算力低的设备上会比稠密模型更快.

我们知道MI50算力不高但显存大带宽高,而llm计算perfill阶段吃算力,decode阶段吃显存带宽,因此我们可以推断出MI50更适合跑moe的思考模型.

### 实验设计

我们根据上面的分类,选择了2个稠密模型和2个moe模型进行测试.每个模型选择4个不同的任务进行测试.每个任务使用相同的prompt进行测试,并测量prefill速度,decode速度和显存占用.

我们以相同的模型,相同的量化等级和相同的上下文长度(尽量占满显存且不溢出)和测试方法对mi50上不同的推理框架进行测试.注意这里测试仅测试推理速度,不测模型能力.

测试样例如下:

> 针对纯文本的embedding模型

由于embedding模型通常的使用场景是先拆分再embedding,再怎么拆分字数也不会超过一篇短篇小说,因此我们选择了4个不同长度的文本进行测试:

+ 诗歌级别(百字内)embedding输出速度,使用`咏鹅`(18个字)作为输入
+ 散文级别(千字范围)embedding输出速度,使用`从百草园到三味书屋`(2500字)作为输入
+ 章节级别(万字范围)embedding输出速度,使用`拖延心理学第一章`(8400字)作为输入

> 针对多模态的直答模型

这类模型在推理速度上主要受prefill阶段的影响,也就是说测试的是不同模型不同推理引擎对算力的利用效率.

我们选择了4个不同的任务进行测试:

+ 识图任务

prompt:

```txt
这张图上有什么内容?请详细描述.
```

图片: ![image](https://i1.hdslb.com/bfs/archive/28d40ebddc4e925b018023f33c175216885fd170.jpg)

+ 看图说故事任务

prompt:

```txt
请根据这张图片编一个有趣的故事.
```

![image](https://i1.hdslb.com/bfs/archive/28d40ebddc4e925b018023f33c175216885fd170.jpg)

+ 图片文字提取任务

prompt:

```txt
图片中的文字内容是什么?请帮我提取出来.
```

![image](https://img2.baidu.com/it/u=4097561749,3332363252&fm=253&fmt=auto&app=138&f=JPEG?w=800&h=1268)

+ 初中几何题作答

prompt:

```txt
图片中有一道题,请帮我解答这道题.
```

![image](https://img2.baidu.com/it/u=4289736232,4144866783&fm=253&fmt=auto&app=138&f=JPEG?w=889&h=500)

> 针对纯文本的思考模型

这里的实验会主要测试decode阶段的速度和显存占用.同时测试在不同上下文长度下的表现.

+ 初中代数题作答

prompt:

```txt
解答下面这道题:
解方程: `$$\frac{2x - 1}{3} - \frac{x + 2}{2} = 1 - \frac{1}{6}x$$`
```

+ 高考命题作文

prompt:

```txt
题目：家国情怀
阅读下面的材料，根据要求写作
他想要给孩子们唱上一段，可是心里直翻腾，开不了口。
——老舍《鼓书艺人》（见全国一卷阅读II）
假如我是一只鸟，
我也应该用嘶哑的喉咙歌唱
——艾青《我爱这土地》
我要以带血的手和你们一一拥抱，
因为一个民族已经起来
——穆旦《赞美》
以上材料引发了你怎样的联想和思考？请写一篇文章。
要求：选准角度，确定立意，明确文体，自拟标题；不要套作，不得抄袭；不得泄露个人信息；不少于800字。
```

+ 简单前端编程题

prompt:

```txt
在网页上显示一个能够实时更新的时钟,要求有秒针,并且能够根据用户选择的时区显示对应的时间,并可以展示日期,星期几和上午下午.你可以用画板或者其他方式来实现这个时钟.
```

+ 任务执行计划定制

prompt:

```txt
**角色和目标：**
你是一名高效、细致的任务规划专家。你的任务是为我提供的项目制定一个详细、分阶段、可执行的计划。

**任务：**
为我规划一个春节假期的旅行计划。假设旅行时间为7天，目的地是日本东京和京都，预算为每人5000美元。计划应包括交通、住宿、饮食、景点游览和购物等方面的安排。

**规划要求：**
1. **分解：** 将整个任务分解为至少 4 个主要的**阶段 (Phase)**。
2. **细化：** 每个阶段下，再细分出 3-5 个具体的**行动步骤 (Action Step)**。
3. **内容：** 每个行动步骤必须包含以下要素：
    * **步骤名称**
    * **描述：** 简要说明如何执行此步骤。
    * **所需资源：** 完成此步骤需要的工具、资料或软件。
    * **前置条件：** 在开始此步骤之前必须完成的步骤（如无，则写“无”）。
    * **预估时间：** 完成此步骤大致需要的时间（例如：2小时，1天）。
4. **格式：** 请使用 Markdown **分层列表**和**粗体**来输出你的计划，以确保清晰和易读。

**思考环节（CoT）：**
在开始规划之前，请先简要思考并写下完成这项任务的 **3 个关键挑战** 和 **3 个衡量成功的标准**。

**开始规划。**
```

+ 文档总结

prompt:

```txt
**系统提示：**
你是一个专业的知识提炼架构师。你的目标是将下面提供的“原始知识片段”压缩和重构为一个高度简洁、结构化的“简单映象”。这个映象将作为主Agent的高级概述，帮助其快速理解知识库的**核心概念和主要结构**，以便进行高效的决策和检索。

**限制：**
1.  输出必须是完整的JSON格式，并且内容长度应尽量控制在512个Token以内。
2.  请忽略原始文本中的日期、作者、版本号（除非是关键区别）、以及任何不重要的叙述性内容。只保留核心事实。

**提炼指令：**
1.  识别知识中最重要的实体（如项目名、模块名、策略名）。
2.  为每个实体提供一个极其简洁的定义或目的。
3.  识别实体之间存在的直接关系、依赖或冲突，并列在 `related` 字段中。

**输入：原始知识片段**
---
拖延的根源
当我们询问拖延者，是什么因素将他们带到了拖延这条路上的，他们一般会这样告诉我们：“我们生活在一个竞争社会！无论何时，别人都期待你有完美的表现。面对这么多的压力，我们无法及时应对。”无疑，在我们通向成功的道路上，我们的确处于各种各样要求的轰炸之中，这占据了我们全部的时间和空间。在我们这个时代，成功的定义是拥有金钱、权力、声誉、美貌和才华——拥有所有这一切。简而言之，成功可以用完美这个词加以定义。不过，它的潜台词却是：“如果你没有拥有所有这一切，那么你肯定哪里出了问题。”在现代社会的匆忙步伐中，在事事的高标准、严要求中，我们中很多人躲进了拖延的避风港，这难道有什么不可理解的吗？
但是除了身处高压力、高要求的社会这个原因之外，一定还有其他原因造成了一个人的拖延。如果身处现代社会是拖延唯一的原因的话，那么每个人就都成了拖延者了。也有很多人，在面对社会压力的时候呈现出其他一些苦恼的症状，比如工作狂、抑郁症、酗酒、吸毒上瘾和恐惧症；同时也有人在全天候的压力下却能茁壮成长。
为了理解你为什么会选择拖延，作为你应对压力的基本策略，我们必须到你个人生活中寻找原因。我们希望你考虑一下它在你的生活中是什么时候开始出现的。

早期记忆
你还记得自己的第一次拖延吗？那是在一个什么样的环境下发生的？
你是在学业上，还是在父母叫你做的事情上拖拖拉拉？那时你多大？是在读小学还是中学……？甚至更早？后来结果怎样？你当时的感受又是什么？我们这里有一些拖延者向我们描述他们早期记忆的例子：
我记得那是在小学二年级的时候，我们第一次被布置写一篇作文。我们要写两个段落的山景，第二天交上去。我记得，老师布置这个作业的时候，我就很害怕。我该写些什么呀？整个夜晚，我都在为此担心，但是我还是没写。最后，在第二天吃早饭的时候，我妈妈帮我写了这篇作文，我抄了一遍交了上去。那个时候，我如释重负。但是，我也感觉自己是一个说谎者。后来，那篇作文我得了个“优秀”。
我不能确切地记起任何事，只是有一些模糊的印象，好像我妈叫我去做什么事情，但是我不想做。
我以前常常匆匆忙忙做完我的家庭作业，好早点出去玩。在我出去之前，我父亲要检查一遍。他总是会找出一些我做得不对的地方，我不得不重做一遍；或者他会自己另外给我布置一些作业。后来，我终于意识到，我作业做得多快多好都没有关系，他只是想让我忙起来，这样我就没有时间出去了，除非他允许。从此之后，我就不想快点做完作业了，我只是坐在那里胡思乱想混时间。
五年级的时候，我在学校里表现很好，所有的老师都喜欢我。那一年，我班的一群女生成立了一个俱乐部，她们不让我加入，就因为我是老师的宠儿。她们取笑我是乖乖女，我觉得这成了我的一个污点。我记得当时我作了一个决定：我永远都不想再成为老师的宠儿了，所以我不再努力学习，并且开始拖延，就是这样。
对于很多人来说，拖延的早期症状都发生在学校里——这是一个年幼的孩子，进人我们这个竞争大社会的第一个台阶。许多学校都把学习能力作为区分学生表现好坏的主要因素，所以你或许会将自己看成班里的一个A等生、C等生或者F等生。学校里的社交派系通常也是建立在这些区分上的。所以，那些头脑分子、体育分子和派对分子，可能会捉弄其他派系的学生，以确立自己在等级上的优越感。同龄人怎么看你，对一个人在学习上和社交上的自信有很大的影晌。即便在离开学校很多年之后，很多成年人还是会以他们孩提时获得的头衔来认识自己。
人们可能会一直为自己在学校里存在的一些学习问题耿耿于怀——阅读障碍，不会做数学题，注意力涣散，信息处理有问题，或者演讲问题。虽然多年来他们可能已经提高了在这些方面的技能，但是他们还是没有感到彻底放心，他们害怕有人会发现他们的这些缺陷。拖延很可能成了他们掩盖自己弱项的一个心理策略。
很可能拖延给了你在教室里不会当众出丑的保护。你的老师最多只能说：“我希望你多努力一点。”但是决不会说：“你缺乏这方面的能力。”因为你的老师永远看不到你所具有的能力。很不幸，人们有时候忘了成绩并不仅仅是对智力的测量，成绩也是对孩子是否专注、是否具有合作能力以及是否具有自由的想象力的一种衡量。
无论你从什么时候开始拖延，一旦开始，就很难停止。拖延除了作为一种自我保护的心理策略之外，它也是源于你对生活的一些顽固信念。我们一再地听到这样的一些观念，我们将它们称为“拖延者的信条”。

拖延者的信条
我必须要做到完美。
我做每件事都应该轻而易举，不费力气。
什么也不做，要比冒失败的风险更为安全。
没有什么是我无法做到的。
如果不能把事情做好，那么它就根本不值得去做。
我必须避开挑战。
如果我成功，有人就会受到伤害。
如果这一次我做得很好，那么我每次都应该做得很好。
按照别人的规定做事，意味着屈服和失去掌控。
我不能承受失去任何人或任何事物。
如果我展现真实的自己，人们不会喜欢我的。
总有一个正确答案，我将一直等待直到发现它。
这些假设你可能听上去很熟悉，也可能你并不知情，它们是通过你的潜意识运作的。不管怎样，它们并不是绝对真理；它们是为拖延开道的一些个人观念。如果你认为必须做到完美，那么你宁愿拖延着也不愿意去努力做事，不愿意冒风险被人评判你的失败。如果你相信成功是危险的，那么你就会通过拖延保护自己和他人，降低自己把事情做成的概率。如果你将合作等同于屈服，那么你就会一直把事情拖着，直到觉得你已经准备好了才去做它，以此来维护自己的掌控感；或者，如果你相信人们不会喜欢真实的你，那么你就会利用拖延保留你自己的想法并跟人们保持一个安全距离。
“拖延者信条”里的这些信念，反映了拖延者的一种阻止自己取得进展的思维方式。自责、害怕以及灾难性的想法让他们不可能越过日常生活中不可避免的困难。意识到你的思维方式是不现实的，这是克服拖延的第一步，虽然“拖延者信条”远不仅仅只是一些不现实的想法。
我们认为，人们之所以产生拖延的不良习性，是因为他们害怕。他们害怕如果他们行动了，他们的行为会让他们陷入麻烦。他们担心如果展示了自己真实的一面，会有危险的结果等着他们。在所有无序和拖拉的背后，他们其实在害怕他们不被接受，以至于他们不仅躲开这个世界，甚至还躲开他们自己。虽然要忍受自责、自轻和对自己的反感是相当痛苦的，但是，比起看清真实的自我所带来的脆弱和无地自容，这样的感受或许更能够被承受得起，拖延是保护他们的盾牌。
---

**输出格式要求：**
请严格按照以下JSON结构返回压缩映象。
{
  "summary_type": "knowledge_map_gist",
  "main_topics": [
    {
      "name": "[核心实体名词]",
      "definition": "[该实体的简洁定义或功能]",
      "related": ["[与之有重要关联的其他实体名词1]", "[与之有重要关联的其他实体名词2]"]
    }
    // ... 更多核心实体
  ],
  "global_summary": "[用一句话总结这些知识片段的最高层主题]"
}

```

### 实验记录

> 多模态模型

+ 识图任务

| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |


+ 看图说故事任务


| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |


+ 初中几何题作答


| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |

+ 图片文字提取任务


| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |

> 纯文本思考模型

+ 初中代数题作答


我们选择`qwen3:30b-a3b-thinking-2507-q4_K_M`作为moe模型的测试对象,并设置上下文长度为`128k`.测试方法是使用相同的prompt进行推理,并且测量推理速度和显存占用.

| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |

+ 高考命题作文


我们选择`qwen3:30b-a3b-thinking-2507-q4_K_M`作为moe模型的测试对象,并设置上下文长度为`128k`.测试方法是使用相同的prompt进行推理,并且测量推理速度和显存占用.

| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |

+ 简单前端编程题


我们选择`qwen3:30b-a3b-thinking-2507-q4_K_M`作为moe模型的测试对象,并设置上下文长度为`128k`.测试方法是使用相同的prompt进行推理,并且测量推理速度和显存占用.

| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |

+ 任务执行计划定制


我们选择`qwen3:30b-a3b-thinking-2507-q4_K_M`作为moe模型的测试对象,并设置上下文长度为`128k`.测试方法是使用相同的prompt进行推理,并且测量推理速度和显存占用.

| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |


+ 文档总结


| Framework                        | Pre-Fill Speed (tokens/s) | Decode Speed | VRAM Usage |
| -------------------------------- | ------------------------- | ------------ | ---------- |
| llama.cpp (Vulkan from lmstudio) | ---                       | ---          | ---        |
| vllm (ROCm from nlzy)            | ---                       | ---          | ---        |
| lmdeploy (ROCm)                  | ---                       | ---          | ---        |

