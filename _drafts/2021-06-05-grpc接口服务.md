---
layout: post
title: "GRPC接口服务"
date: 2021-06-04
author: "Hsz"
category: introduce
tags:
    - WebTech
header-img: "img/home-bg-o.jpg"
update: 2021-06-04
---
# GRPC接口服务

[GRPC](https://grpc.io/)正如其名,是目前应用较广的一种RPC(Remote Procedure Call)协议.

<!--more-->

## RPC

RPC(Remote Procedure Call),远程过程调用.它的设计目标就是希望调用起来和使用本地的函数一样简单.也就是说RPC只是一种形式,本质上还是一种请求响应模式的服务.

RPC技术由来已久,也很早就被应用,比如jsonrpc,xmlrpc这些.但多数RPC技术也有一些顽疾:

1. 语言绑定

    RPC技术既然是让远程调用像本地调用一样使用的技术,那必然会和"本地调用"的语言绑定.几乎没有一个RPC技术可以说是语言无关的,只是支持的语言多少的问题.

2. 本地调用和远程调用并不相同

    RPC的核心想法是隐藏远程调用的复杂性,但是很多RPC的实现隐藏得有些过头了,而会造成一些问题.
    1. 使用本地调用不会引起性能问题,但是RPC会花大量的时间对负荷进行序列化和反序列化,更别提网络通信所需要的时间.这意味着要使用不同的思路来设计远程和本地的API(需要考虑rpc的服务端是否可用等).简单地把一个本地的API改造成为跨服务的远程API往往会带来问题.
    2. 由于隐藏的太好开发人员会在不知道该调用是远程调用的情况下对其进行使用.这会进一步放大上面的问题
    3. 数据冗余,如果一个函数它的返回值有数据冗余,我们往往还是可以到处复用的,因为除了一点内存外几乎没有成本,但RPC不同它的返回和函数一样也是固定的,所以冗余数据会沿着网络整个传过来,这就造成了序列化反序列化时cpu内存的浪费以及网络带宽的浪费.成本就高了

上面说这么多当然不是说RPC不好,恰恰相反,RPC是非常好用的工具,在拆分服务时改动最小,而且也最好维护.所有大厂都会使用RPC技术,很多开源项目如tensorflow都会有rpc的应用.

## GRPC

GRPC是谷歌开源出来的一个RPC协议,目前官方有3大类实现

1. C实现
2. golang实现
3. java实现

而C实现又绑定了大量其他语言的接口,比如python,node,C#,C++等.从使用的技术上来说它使用HTTP2协议作为传输协议,protobuf作为序列化协议.加上社区一直以来的优化,总体而言GRPC在各种RPC协议中性能属于上游水平.但与其说GRPC是一个RPC技术,不如说它是一套解决方案.GRPC官方实现了一整套包括反射,健康检测,负载均衡和服务发现,debug性能调优等在内的工具和模块,无比庞大无比复杂.老实说想都整明白还是有点费劲的.本文的目的就是把GRPC和与其配套的技术整体理一理,并给出一个相对通用的使用模板来.

不过由于我不会java,所以本文只会介绍C实现(以python为例)和Go实现.

## 基本使用

GRPC的基本使用流程是:

1. 服务端与客户端开发者协商创建一个protobuf文件用于定义rpc的形式和方法名以及不同方法传输数据的schema
2. 客户端和服务端分别编译这个protobuf文件为自己需要的目标语言的模块
3. 服务端:
    1. 导入protobuf文件编译成的模块
    2. 继承定义service服务类(go语言是自己创建类)
    3. 实现service服务类其中定义的方法
    4. 这个类的一个实例注册到grpc服务器
    5. 启动grpc服务器提供服务
4. 客户端:
    1. 导入protobuf文件编译成的模块
    2. 创建通讯连接(C实现中叫`Channel`,go实现中叫`ClientConn`)
    3. 使用这个通讯连接实例化protobuf文件编译成的模块中的service客户端类
    4. 用这个service客户端类的实例发送请求获得响应

一个典型的rpc定义proto文件如下:

```protobuf
syntax = "proto3";
package test.foo;
option go_package = "./foo";

service Bar {
    rpc Square (Message) returns (Message){}
    rpc RangeSquare (Message) returns (stream Message){}
    rpc SumSquare (stream Message) returns (Message){}
    rpc StreamrangeSquare (stream Message) returns (stream Message){}
}
message Message {
    double Message = 1;
}
```

可以看出grpc的声明实际上时protobuf语法的一个扩展,新增了如下关键字来描述一个grpc服务

| 关键字    | 说明                                       |
| --------- | ------------------------------------------ |
| `service` | 申明定义的是一个grpc的Service              |
| `rpc`     | 申明这一行定义的是服务下的一个远程调用方法 |
| `returns` | 声明本行定义的`rpc`的返回值形式            |
| `stream`  | 声明这个数据是个流数据                     |

### rpc的形式

grpc总体来讲还是传统的请求响应模式,不同之处在于借助http2协议,grpc支持4种形式的请求响应

+ 请求-响应,最常见的单请求单响应,应用最广,对应上面的`rpc Square (Message) returns (Message){}`,在python中相当于调用函数
+ 请求-流响应,单请求流响应,适用于响应数据量过大或者替代订阅监听模式,对应上面的`rpc RangeSquare (Message) returns (stream Message){}`,在python中相当于调用迭代器
+ 流请求-响应,流请求单响应,适用于请求数据量过大或者替代推拉模式,对应上面的`rpc SumSquare (stream Message) returns (Message){}`,在python中相当于参数是可迭代对象的函数
+ 流请求-流响应,流请求流响应,适用于替代双工通信或者一些批处理场景,对应上面的`rpc StreamrangeSquare (stream Message) returns (stream Message){}`,相当于使用生成器(调用next方法和throw方法).

grpc的一大特色就是支持这四种形式的请求响应.传统rpc一般只支持第一种`请求-响应`模式,只有少部分可能会支持第二种`请求-流响应`.调用形式的丰富也让grpc可以适用于各种场景.

### 获取元数据

grpc除了可以通过请求消息和响应消息传递数据,还可以通过"元数据"传递消息.元数据角色有点类似http协议中的http头.grpc中的元数据分为`header`和`trailer`,他们都是纯字符串的键值对,都可以用于传输数据只是发送的时间不同.

在请求-响应模式中,我们的请求和响应数据发送顺序如图

![单向meta数据](../img/in-post/grpc/单向meta.png)

而在流式响应中发送顺序如下图

![流meta数据](../img/in-post/grpc/流meta.png)

由此可以看出

1. 请求只有头没有`trailer`
2. 如果是单纯的请求响应,那么`header`和`trailer`可以认为没有区别,但对于流数据,`header`是在流的开始时发送的,而`trailer`则是在流结束时发送的.

无论是`header`还是`trailer`我们要用meta数据无非四个场景

1. 客户端设置meta数据发给服务端
2. 服务端接收meta数据
3. 服务端设置meta数据给客户端
4. 客户端接收meta数据

#### Go实现中的meta数据设置

go实现中使用包`"google.golang.org/grpc/metadata"`管理元数据. 无论是`header`还是`trailer`都是`metadata.MD`类型.

在客户端请求时我们将其待在ctx上发送请求,而服务端也是在ctx上获取meta数据;
服务端则是通过grpc或者stream中专门的方法来设置发送meta数据,而客户端则是在stream中获取.

下面是四个场景下的示例代码

> 客户端设置meta数据发给服务端

+ 对于简单请求

    ```golang
    md := metadata.Pairs("timestamp", time.Now().Format(timestampFormat))
    ctx := metadata.NewOutgoingContext(context.Background(), md)
    r, err := c.UnaryEcho(ctx, &pb.EchoRequest{Message: message}
    ```

+ 对于请求流

    ```golang
    md := metadata.Pairs("timestamp", time.Now().Format(timestampFormat))
    ctx := metadata.NewOutgoingContext(context.Background(), md)
    stream, err := c.ClientStreamingEcho(ctx)
    ```

> 服务端接收meta数据

+ 对于简单请求

    ```golang
    md, ok := metadata.FromIncomingContext(ctx)
    ```

+ 对于请求流

    ```golang
    md, ok := metadata.FromIncomingContext(stream.Context())
    ```

> 服务端设置meta数据给客户端

+ 对于简单响应

    ```golang
    defer func() {
        trailer := metadata.Pairs("timestamp", time.Now().Format(timestampFormat))
        grpc.SetTrailer(ctx, trailer)
    }()

        ...

    // Create and send header.
    header := metadata.New(map[string]string{"location": "MTV", "timestamp": time.Now().Format(timestampFormat)})
    grpc.SendHeader(ctx, header)
    ```

+ 对于流响应

    ```golang
    defer func() {
        trailer := metadata.Pairs("timestamp", time.Now().Format(timestampFormat))
        stream.SetTrailer(trailer)
    }()

    ...

    // Create and send header.
    header := metadata.New(map[string]string{"location": "MTV", "timestamp": time.Now().Format(timestampFormat)})
    stream.SendHeader(header)
    ```

> 客户端接收meta数据

+ 对于简单响应

    ```golang
    var header, trailer metadata.MD
    r, err := c.UnaryEcho(ctx, &pb.EchoRequest{Message: message}, grpc.Header(&header), grpc.Trailer(&trailer))
    ```

+ 对于流响应

    ```golang
    header, err := stream.Header()

    trailer := stream.Trailer()
    ```

#### C实现中的meta数据设置

在C实现中meta则是以元组的形式传递.


下面是四个场景下的示例代码

> 客户端设置meta数据发给服务端

+ 对于简单请求

    ```python
    md := metadata.Pairs("timestamp", time.Now().Format(timestampFormat))
    ctx := metadata.NewOutgoingContext(context.Background(), md)
    r, err := c.UnaryEcho(ctx, &pb.EchoRequest{Message: message}
    ```

+ 对于请求流

    ```python
    md := metadata.Pairs("timestamp", time.Now().Format(timestampFormat))
    ctx := metadata.NewOutgoingContext(context.Background(), md)
    stream, err := c.ClientStreamingEcho(ctx)
    ```

> 服务端接收meta数据

+ 对于简单请求

    ```python
    md, ok := metadata.FromIncomingContext(ctx)
    ```

+ 对于请求流

    ```python
    md, ok := metadata.FromIncomingContext(stream.Context())
    ```

> 服务端设置meta数据给客户端

+ 对于简单响应

    ```python
    defer func() {
        trailer := metadata.Pairs("timestamp", time.Now().Format(timestampFormat))
        grpc.SetTrailer(ctx, trailer)
    }()

        ...

    // Create and send header.
    header := metadata.New(map[string]string{"location": "MTV", "timestamp": time.Now().Format(timestampFormat)})
    grpc.SendHeader(ctx, header)
    ```

+ 对于流响应

    ```python
    defer func() {
        trailer := metadata.Pairs("timestamp", time.Now().Format(timestampFormat))
        stream.SetTrailer(trailer)
    }()

    ...

    // Create and send header.
    header := metadata.New(map[string]string{"location": "MTV", "timestamp": time.Now().Format(timestampFormat)})
    stream.SendHeader(header)
    ```

> 客户端接收meta数据

+ 对于简单响应

    ```python
    var header, trailer metadata.MD
    r, err := c.UnaryEcho(ctx, &pb.EchoRequest{Message: message}, grpc.Header(&header), grpc.Trailer(&trailer))
    ```

+ 对于流响应

    ```python
    header, err := stream.Header()

    trailer := stream.Trailer()
    ```

### 网络参数设置

grpc毕竟还是rpc,网络是绕不开的话题.网络的设置直接影响调用的性能,grpc使用http2作为网络传输协议,因此有不少用于微调http2的设置项,而且这些设置项有服务端用的,也有客户端服务端都可以用的,我整理了常用的设置项如下:

| 设置说明                                                            | 使用方 | C实现中的字段                         | GO实现中的字段                                                                                     |
| ------------------------------------------------------------------- | ------ | ------------------------------------- | -------------------------------------------------------------------------------------------------- |
| 允许接收的最大消息长度                                              | c&s    | `grpc.max_receive_message_length`     | `grpc.MaxRecvMsgSize(MaxRecvMsgSize)`                                                              |
| 允许发送的最大消息长度                                              | c&s    | `grpc.max_send_message_length`        | `grpc.MaxSendMsgSize(MaxSendMsgSize)`                                                              |
| 基于Stream的滑动窗口大小                                            | c&s    | `grpc.http2.lookahead_bytes`          | `grpc.InitialWindowSize(InitialWindowSize)`                                                        |
| 基于Connection的滑动窗口大小                                        | c&s    | ---                                   | `grpc.InitialConnWindowSize(InitialConnWindowSize)`                                                |
| 一个连接中最大并发Stream数                                          | c&s    | `rpc.max_concurrent_streams`          | `grpc.MaxConcurrentStreams(s.MaxConcurrentStreams)`                                                |
| 空闲连接每隔n秒ping一次客户端已确保连接存活                         | c&s    | `grpc.keepalive_time_ms`              | `keepalive.ClientParameters.Time`/`keepalive.ServerParameters.Time`                                |
| ping时长超过n则认为连接已死                                         | c&s    | `grpc.keepalive_timeout_ms`           | `keepalive.ClientParameters.Timeout`/`keepalive.ServerParameters.Timeout`                          |
| 即使没有活动流也允许ping                                            | c&s    | `grpc.keepalive_permit_without_calls` | `keepalive.ClientParameters.PermitWithoutStream`/`keepalive.EnforcementPolicy.PermitWithoutStream` |
| 客户端连接的最大空闲时长                                            | s      | `grpc.max_connection_idle_ms`         | `keepalive.ServerParameters.MaxConnectionIdle`                                                     |
| 如果连接存活超过n则发送goaway                                       | s      | `grpc.max_connection_age_ms`          | `keepalive.ServerParameters.MaxConnectionAge`                                                      |
| 强制关闭连接之前允许等待的rpc在n秒内完成                            | s      | `grpc.max_connection_age_grace_ms`    | `keepalive.ServerParameters.MaxConnectionAgeGrace`                                                 |
| 如果客户端超过每n秒ping一次则终止连接                               | s      | ---                                   | `keepalive.EnforcementPolicy.MinTime`                                                              |
| 服务端的性能偏向,支持`latency`低延迟;`blend`均衡,`throughput`高吞吐 | s      | `grpc.optimization_target`            | ---                                                                                                |

### 数据压缩设置

grpc支持使用数据压缩技术,这会一定程度上增加cpu负载,但会降低通信的带宽要求.

grpc的数据支持3种类型:

1. `grpc.Compression.NoCompression`即不压缩
2. `grpc.Compression.Deflate`即使用`Deflate`算法压缩
3. `grpc.Compression.Gzip`使用gzip算法压缩

在如何处理压缩上,C实现和go实现的思路是不同的.

#### go实现的数据压缩设置

go实现的思路是

> 客户端需要指定使用的压缩类型将请求发送给服务端,而服务端则根据请求指定的压缩类型解析数据,在完成请求后将响应的数据按客户端指定的压缩类型压缩后发送给客户端.

##### 客户端数据压缩

客户端指定压缩类型需要在调用grpc定义的rpc时指定

```golang
import (
    ...
    "google.golang.org/grpc/encoding/gzip" // Install the gzip compressor
    ...
)
...
res, err := c.UnaryEcho(ctx, &pb.EchoRequest{Message: msg}, grpc.UseCompressor(gzip.Name))
```

如果所有的请求都需要使用指定的压缩方式,我们也可以在创建连接时指定.

```golang
...
grpc.Dial(address,grpc.WithDefaultCallOptions(grpc.UseCompressor(gzip.Name)))
...
```

##### 服务端数据压缩

服务端只要注册了解压算法就无需再设置什么了

```golang
_ "google.golang.org/grpc/encoding/gzip" 
...
```

#### C实现的数据压缩设置

而C实现的思路则是

> 客户端指定压缩类型只是告诉服务端如何解压,而服务端可以自己设置响应数据的压缩类型

##### 客户端数据压缩

客户端指定压缩类型需要在调用grpc定义的rpc时指定

```python
response = stub.SayHello(helloworld_pb2.HelloRequest(name='you'),
                        compression=grpc.Compression.Deflate)
```

如果所有的请求都需要使用指定的压缩方式,我们也可以在创建连接时指定.

```python
with grpc.insecure_channel('foo.bar:1234', compression=grpc.Compression.Gzip) as channel:
    use_channel(channel)
```

##### 服务端数据压缩

服务端数据压缩一样是在调用这层,我们可以在实现rpc时通过设置`context`来实现

```python
def SayHello(self, request, context):
    context.set_response_compression(grpc.Compression.NoCompression)
    return helloworld_pb2.HelloReply(message='Hello, %s!' % request.name)
```

如果所有的响应都使用相同的压缩类型,可以在构造server时直接指定

```python
server = grpc.server(futures.ThreadPoolExecutor(),
                     compression=grpc.Compression.Gzip)
```

通常情况我们的grpc都是用于构造计算密集型任务用的,cpu资源是比较稀缺的资源,而且grpc使用的protobuf作为一种序列化协议来说已经很紧凑,因此一般不会设置压缩.

### TLS

既然grpc是基于http2的那它自然也就支持TLS.

| 场景         | 例子                                                          |
| ------------ | ------------------------------------------------------------- |
| C实现客户端  | `grpc.secure_channel(addr,credentials,...)`                   |
| C实现服务端  | `server.add_secure_port(addr, credentials)`                   |
| Go实现客户端 | `grpc.Dial(addr, grpc.WithTransportCredentials(credentials))` |
| Go实现服务端 | `grpc.NewServer(grpc.Creds(credentials))`                     |

我们只需要根据需要设置好`credentials`就可以了

### 获取元数据



### 拦截器



### 客户端本地负载均衡(仅限go和java实现)



## 使用插件扩展GRPC的能力

### 反射插件

### 健康检测插件

### 服务观察插件

