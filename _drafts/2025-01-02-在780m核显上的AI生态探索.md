---
layout: post
title: "在780m核显上的AI生态探索"
series:
    aipc_experiment:
        index: 4
date: 2025-01-02
author: "Hsz"
category: experiment
tags:
    - Linux
    - AIPC
    - Rocm
    - AIGC
    - LLM
    - TTS
    - Voice
    - Stable Diffusion
header-img: "img/home-bg-o.jpg"
update: 2025-01-02
---
# 在780m核显上的AI生态探索

780m这颗核显在笔记本平台基本已经证明了它的实力--默频约等于1050ti,小超约等于1650,极限超可以摸到1060屁股.我们且不考虑性能.先让它可以被机器学习相关工具调用起来.这也算是一窥amd的AI相关生态.

我们的验证平台是8700g,ubuntu 20.04.在前文中已经安装好了驱动和rocm,现在正式开始探索之旅.

## pytorch支持

我们专门创建一个mamba环境来验证pytorch的支持

1. 在`~/mambaenvs`目录下创建`py3_10_rocm.yml`文件

    ```yml
    name: py3.10_rocm
    channels:
    - conda-forge
    dependencies:
    - python ~=3.10.11
    - jupyter
    - jupyterlab
    - ipywidgets
    - ipyparallel
    ```

    目前`py3.10`是ai生态支持比较广泛的python版本,我们以它为基准版本

2. 创建mamba虚拟环境

    ```bash
    mamba create -f py3_10_rocm.yml
    ```

3. 激活环境安装绑定rocm的pytorch

    ```bash
    mamba activate py3.10
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.2
    ```

4. 验证安装成功

    进入python交互环境,执行

    ```python
    import torch
    x = torch.rand(5, 3)
    print(x)
    ```

    打印出类似

    ```pyhton
    tensor([[0.3380, 0.3845, 0.3217],
        [0.8337, 0.9050, 0.2650],
        [0.2979, 0.7141, 0.9069],
        [0.1449, 0.1132, 0.1375],
        [0.4675, 0.3947, 0.1426]])
    ```

    的内容说明pytorch安装没有问题.

    执行

    ```python
    torch.cuda.is_available()
    ```

    返回`True`说明rocm可用

5. 验证效果

    随便找个目录,拉个例子来跑下,验证下gpu确实被调用

    ```bash
    cd ~/workspace/test
    git clone https://github.com/pytorch/examples.git
    cd examples/mnist
    setproxy # 注意得挂代理
    python main.py
    ```

    打开`任务中心`查看GPU占用,如果能用起来说明rocm确实的被打开了

## docker支持

amd提供了一系列docker镜像
