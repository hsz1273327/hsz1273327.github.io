---
layout: post
title: "在780m核显上的AI生态探索"
series:
    aipc_experiment:
        index: 4
date: 2025-01-02
author: "Hsz"
category: experiment
tags:
    - Linux
    - AIPC
    - Rocm
    - AIGC
    - LLM
    - TTS
    - Voice
    - Stable Diffusion
header-img: "img/home-bg-o.jpg"
update: 2025-01-02
---
# 在780m核显上的AI生态探索

780m这颗核显在笔记本平台基本已经证明了它的实力--默频约等于1050ti,小超约等于1650,极限超可以摸到1060屁股.我们且不考虑性能.先让它可以被机器学习相关工具调用起来.这也算是一窥amd的AI相关生态.

我们的验证平台是8700g,ubuntu 24.04.在前文中已经安装好了驱动和rocm,现在正式开始探索之旅.

## 核显ai生态的基础

我们都知道核显的显存就是内存,一般正常的主板bios最多给你分配16g内存作为核显的显存.当然16g并不算少,但对于很多情况来说也不多.好在[linux在内核版本`6.10`开始允许为核显分配更多的内存作为GTT内存参与核显运算](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=v6.10-rc4&id=eb853413d02c8d9b27942429b261a9eef228f005)

这里解释下现代核显的内存模型.对于核显来说,它并没有自己的显存(`vram`),因此只能从物理内存中"划一块"当作显存使用.因此我们的物理内存就有了两块组成`vram`(核显显存)和`ram`(内存).这两块虽然在物理层面是一样的,但使用时井水不犯河水--`vram`是核显专用,`ram`是cpu专用,这两者由于运作机制不同,数据封装等都不相同,因此即便是想也无法直接混用.但很多时候显存并不够用,这时我们就会希望要是能从`ram`(内存)再分点显存用用就好了.这个再分点给显存的部分就是[GTT内存](https://en.wikipedia.org/wiki/Graphics_address_remapping_table).`GTT`一旦被划分出去那就和`vram`一样不再属于`ram`了也就是成了核显专用的了.`GTT`和`vram`在使用时大体上是没有区别的,他们之间的区别主要是

| 区别 | `VRAM`                   | `GTT`                               |
| ---- | ------------------------ | ----------------------------------- |
| 来源 | 由bios设置划分           | 由操作系统划分                      |
| 性能 | 显示需要的内存性能会更好 | 由于无法直达Framebuffer因此会略差些 |

linux内核的这一特性默认会为`vram`和`GTT`一共划分一半的内存,而且这个容量是可以设置的,比如假如我们有64g的内存,我们想划分48g给`GTT`用可以通过编辑`/etc/modprobe.d/ttm.conf`来调整(以4k页为单位对于48G来说就是)

```txt
ttm pages_limit=12582912
ttm page_pool_size=12582912
```

因此我们完全可以在bios中的将`VRAM`设置为`auto`(默认为512m)让核显的`vram`仅用于显示,计算就全靠`GTT`.

这个特性刚出来半年,很多软件并没有很好的适配,但很显然,这种白占大显存的便宜很快就会跟上的.

当然你要说缺陷那自然也是有缺陷的.我们本质上还是在物理内存上划一块给显卡用,和apu最早的愿景--统一内存寻址还是有很大区别.但相比起apple的黄金内存,英伟达的振金显存,这各方案成本太低了.

## 基于官方基座运算库的ai应用

基座运算库有多重要看看老黄赚多少就知道了.cuda早已占据了最好的生态位,这让amd和英特尔的显卡包括核显都很难受.

回到我们的主题,在ubuntu 24.04环境下的780m上我们能用的基座运算库只有[rocm](https://github.com/ROCm/ROCm).考虑到后面还要装pytorch,稳定版最高仅支持到rocm 6.2.4.具体怎么安装可以看上一篇文章中安装驱动的部分.

下面是我总结的官方rocm 6.2.4版本

不过如果打算使用除pytorch/huggingface套件和llama.cp/ollama外的的其他工具,我们就不能装官方rocm和相关工具,而是要借助[lamikr/rocm_sdk_builder](https://github.com/lamikr/rocm_sdk_builder/tree/master)项目,这个我们后面再说.

<!-- 在rocm之上,次一级的基座运算库基本都是依赖于rocm的,他们一般可以在<https://repo.radeon.com/rocm/manylinux/rocm-rel-{rocm版本}>目录下找到 -->

### pytorch的rocm支持

在rocm安装好后我们也是安装对应的pytorch的.

让我们专门创建一个mamba环境来验证pytorch的支持

1. 在`~/mambaenvs`目录下创建`py3_11_rocm.yml`文件

    ```yml
    name: py3.11_rocm
    channels:
    - conda-forge
    dependencies:
    - python ~=3.11.10
    - jupyter
    - jupyterlab
    - ipywidgets
    - ipyparallel
    ```

    目前`py3.11`是ai生态支持比较广泛的python版本,我们以它为基准版本

2. 创建mamba虚拟环境

    ```bash
    mamba create -f py3_11_rocm.yml
    ```

3. 激活环境安装绑定rocm的pytorch

    ```bash
    mamba activate py3.11_rocm
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.2
    ```

4. 验证安装成功

    进入python交互环境,执行

    ```python
    import torch
    x = torch.rand(5, 3)
    print(x)
    ```

    打印出类似

    ```pyhton
    tensor([[0.3380, 0.3845, 0.3217],
        [0.8337, 0.9050, 0.2650],
        [0.2979, 0.7141, 0.9069],
        [0.1449, 0.1132, 0.1375],
        [0.4675, 0.3947, 0.1426]])
    ```

    的内容说明pytorch安装没有问题.

    执行pip install httpx[socks]

    ```python
    torch.cuda.is_available()
    ```

    返回`True`说明rocm可用

5. 验证效果

    我们可以用如下两个官网的例子来验证效果.打开`任务中心`,然后执行下面的例子,查看GPU占用,如果能用起来说明rocm确实的被打开了

> 测试FashionMNIST

```python
import torch
from torch import nn
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor
# 训练
# 下载训练数据
training_data = datasets.FashionMNIST(
    root="data",
    train=True,
    download=True,
    transform=ToTensor(),
)
# 下载测试数据
test_data = datasets.FashionMNIST(
    root="data",
    train=False,
    download=True,
    transform=ToTensor(),
)
# 构造数据加载器
batch_size = 64
train_dataloader = DataLoader(training_data, batch_size=batch_size)
test_dataloader = DataLoader(test_data, batch_size=batch_size)
for X, y in test_dataloader:
    print(f"Shape of X [N, C, H, W]: {X.shape}")
    print(f"Shape of y: {y.shape} {y.dtype}")
    break
# 构造模型
device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)
print(f"Using {device} device")

class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(28*28, 512),
            nn.ReLU(),
            nn.Linear(512, 512),
            nn.ReLU(),
            nn.Linear(512, 10)
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        return logits

model = NeuralNetwork().to(device)
print(model)
# 设置优化器
loss_fn = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)
# 构造训练函数
def train(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    model.train()
    for batch, (X, y) in enumerate(dataloader):
        X, y = X.to(device), y.to(device)

        # Compute prediction error
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        if batch % 100 == 0:
            loss, current = loss.item(), (batch + 1) * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")
# 构造测试函数
def test(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    model.eval()
    test_loss, correct = 0, 0
    with torch.no_grad():
        for X, y in dataloader:
            X, y = X.to(device), y.to(device)
            pred = model(X)
            test_loss += loss_fn(pred, y).item()
            correct += (pred.argmax(1) == y).type(torch.float).sum().item()
    test_loss /= num_batches
    correct /= size
    print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")
# 开始训练,训练5个epoch
epochs = 5
for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train(train_dataloader, model, loss_fn, optimizer)
    test(test_dataloader, model, loss_fn)
print("Done!")
# 保存模型
torch.save(model.state_dict(), "model.pth")
print("Saved PyTorch Model State to model.pth")

# 推理
# 加载模型
model = NeuralNetwork().to(device)
model.load_state_dict(torch.load("model.pth", weights_only=True))
# 开始推理
classes = [
    "T-shirt/top",
    "Trouser",
    "Pullover",
    "Dress",
    "Coat",
    "Sandal",
    "Shirt",
    "Sneaker",
    "Bag",
    "Ankle boot",
]

model.eval()
x, y = test_data[0][0], test_data[0][1]
with torch.no_grad():
    x = x.to(device)
    pred = model(x)
    predicted, actual = classes[pred[0].argmax(0)], classes[y]
    print(f'Predicted: "{predicted}", Actual: "{actual}"')
```

> 测试resnet50推理

```python
import torch
from PIL import Image
from torchvision import transforms

# 加载模型
model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)

# 开始推理
model.eval()
filename = "dog.jpg" # 从<https://github.com/pytorch/hub/raw/master/images/dog.jpg>下载到同文件夹下
input_image = Image.open(filename)
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])
input_tensor = preprocess(input_image)
input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model

# move the input and model to GPU for speed if available
if torch.cuda.is_available():
    input_batch = input_batch.to('cuda')
    model.to('cuda')

with torch.no_grad():
    output = model(input_batch)
# Tensor of shape 1000, with confidence scores over ImageNet's 1000 classes
# print(output[0])
# The output has unnormalized scores. To get probabilities, you can run a softmax on it.
probabilities = torch.nn.functional.softmax(output[0], dim=0)
print(probabilities)

with open("imagenet_classes.txt", "r") as f: # 从<https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt>下载到本地同文件夹下
    categories = [s.strip() for s in f.readlines()]
top5_prob, top5_catid = torch.topk(probabilities, 5)
for i in range(top5_prob.size(0)):
    print(categories[top5_catid[i]], top5_prob[i].item())
```

经过测试pytorch对官方rocm的支持还是可以的,正常使用可能会有一些警告.比如`Attempting to use hipBLASLt on an unsupported architecture! Overriding blas backend to hipblas`可以通过环境变量`TORCH_BLAS_PREFER_HIPBLASLT=0`解决.但可以正常运行大多数接口.
<!-- `DISABLE_ADDMM_CUDA_LT=1` -->

### huggingface的rocm支持

huggingface在仅用到pytorch的场景下也表现良好,但官方宣传的`Flash Attention`是没有的,`GPTQ`也只支持到了`rocm5.7`且基本已经停止了对rocm的支持.致于`onnxruntme`目前只支持到`rocm6.0`,要用得自己编译,我尝试按官方的文档进行编译并不能编译成功.

好在大多数功能来说还是可以使用的,问题没那么大

huggingface相关库我只建议像下面这样安装

```bash
pip install transformers # 主库
pip install accelerate # 加速模型加载
pip install 'diffusers[torch]' # sd专用库
pip install huggingface_hub # 下模型和数据集用的
```

是的,不要安装任何和amd,rocm相关的优化库,亲测没用,不是报错就是根本不起作用.

#### 换源

由于我们在墙内,下模型就太痛苦了,我们可以先换源[hf-mirror](https://hf-mirror.com/)

环境变量

```bash
#========================================================================== huggingface
export HF_ENDPOINT=https://hf-mirror.com # 下载模型的位置
export HF_HUB_CACHE="~/.cache/huggingface/hub" # 制定模型/数据集缓存位置
```

#### 测试可用性

`huggingface`套件主要是用来做推理和finetune的,我们来试试能不能用

> 测试llm推理

1. 先下载需要的大模型,我们以[Qwen/Qwen2-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2-1.5B-Instruct)为例

    ```bash
    huggingface-cli download --resume-download Qwen/Qwen2-1.5B-Instruct --local-dir ~/WorkSpace/Models/Qwen/Qwen2-1.5B-Instruct
    ```

2. 编写测试程序

    ```python
    from transformers import AutoModelForCausalLM, AutoTokenizer
    device = "cuda"
    # 加载模型
    model = AutoModelForCausalLM.from_pretrained(
        "~/WorkSpace/models/Qwen/Qwen2-1.5B-Instruct",
        torch_dtype="auto",
        device_map="auto"
    )
    tokenizer = AutoTokenizer.from_pretrained("~/WorkSpace/models/Qwen/Qwen2-1.5B-Instruct")
    # 推理
    prompt = "和我说说你是谁."
    messages = [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": prompt}
    ]
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    print("模板化后："+text)
    model_inputs = tokenizer([text], return_tensors="pt").to(device)
    print(model_inputs.input_ids)
    # Directly use generate() and tokenizer.decode() to get the output.
    # Use `max_new_tokens` to control the maximum output length.
    generated_ids = model.generate(
        model_inputs.input_ids,
        max_new_tokens=512
    )
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    print("========================================================================")
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    print(response)
    ```

    正常是可以正常对话的,而且可以看到显卡被全量占用.

> 测试sd

1. 先下载sd1.5模型

    ```bash
    huggingface-cli download --resume-download stable-diffusion-v1-5/stable-diffusion-v1-5 --local-dir ~/WorkSpace/Models/stable-diffusion-v1-5/stable-diffusion-v1-5
    ```

2. 执行如下脚本

    ```python
    from diffusers import DiffusionPipeline
    # 加载模型
    pipeline = DiffusionPipeline.from_pretrained("/home/hsz/WorkSpace/Models/stable-diffusion-v1-5/stable-diffusion-v1-5", use_safetensors=True)
    # 推理
    pipeline.to("cuda")
    image = pipeline("An image of a squirrel in Picasso style").images[0]
    image.save("image_of_squirrel_painting.png")
    ```

    正常是可以正常对话的,而且可以看到显卡被全量占用.

## 本地llm

上面的例子我们已经用huggingface做了llm推理,但我们知道huggingface(包括pytorch)本质上是一个通用计算框架,并没有为推理做优化,因此效能并不高.本地llm推理的主流还是专用的llm推理框架.

这其中最主流的是以`llama.cpp`为基座的应用框架. 值得一说的是`llama.cpp`完美支持官方rocm

### llama.cpp

[llama.cpp](https://github.com/ggerganov/llama.cpp)是一个纯c/c++构造的端侧llm推理框架,最早是为了能在macos上高效的跑llm而创建的,越做越好用的人越来越多支持的平台支持的硬件越来越多,现如今已经是最主流的端侧llm推理框架了.

它的主要特点有

+ 支持纯cpu推理(OpenBLAS),尤其如果cpu支持avx512效能不错
+ 支持macos(Metal)
+ 支持amd显卡(rocm/HIP),而且自己编译可以让核显启动HIP_UMA模式,使用全量内存作为显存,当然这样做会影响独显推理性能
+ 支持在windows使用Vulkan作为运算后台.这样只要是支持vulkan的显卡在window上都能顺利使用
+ 支持N卡(cuda),I卡(SYCL)
+ 支持升腾npu(CANN)和摩尔线程显卡(MUSA)
+ 支持混合推理,你可以将大模型按层分配给不同的gpu和cpu串行推理
+ 支持android端侧部署

安装也很简单,我们可以直接用`homebrew`安装

```bash
brew install llama.cpp
```

brew会下载预编译好的llama.cpp,你就可以直接用了,不过这样安装的纯cpu推理的版本.先不考虑性能,我们继续往下.

> 推理

llama.cpp使用[GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)格式的大模型,这种格式对低精度量化的模型非常友好,相对更加紧凑,而且可以包含大量元信息,非常适合在资源受限的端侧部署.这种格式的模型huggingface上有些是有现成的有些则需要自己转,我们就先用一个现成的[Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF](https://huggingface.co/Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF/tree/main)来演示下

```bash
# 下载模型
huggingface-cli download --resume-download Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF --local-dir ~/WorkSpace/Models/Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF
# 命令行聊天
llama-cli -m ~/WorkSpace/models/Qwen/Qwen2.5-Coder-0.5B-Instruct-GGUF/qwen2.5-coder-0.5b-instruct-fp16.gguf -p "你是一个个人助理" -cnv
```

> 编译GPU版的llama.cpp

在机器上要让llama.cpp用上gpu你得编译安装.你可以像下面这样编译.

```bash
# 克隆源码到本地
git clone https://github.com/ggerganov/llama.cpp.git
# 切到最新的release分支
git checkout b4485  
# 开始编译
HIPCXX="$(hipconfig -l)/clang" HIP_PATH="$(hipconfig -R)" cmake -S . -B build -DGGML_HIP=ON -DAMDGPU_TARGETS=gfx1100  -DCMAKE_BUILD_TYPE=Release  && cmake --build build --config Release -- -j 16
```

这里面主要要改的就是`AMDGPU_TARGETS`这个参数,我是780m,因此指定为`gfx1100`.

如果你打算就amd这个核显用到天荒地老也不加独显了可以考虑用下面这种方式自己编译llama.cpp.这样你就可以用全量内存当显存了.

```bash
HIPCXX="$(hipconfig -l)/clang" HIP_PATH="$(hipconfig -R)" cmake -S . -B build -DGGML_HIP=ON -DAMDGPU_TARGETS=gfx1100 -DGGML_HIP_UMA=ON -DCMAKE_BUILD_TYPE=Release  && cmake --build build --config Release -- -j 16
```

个人还是推荐第一种编译方式,因为780m核显虽然挺不错的,但和独显还是没法比,还是预留下给独显的空间比较好.

亲测对于qwen2.5-coder-0.5b-instruct-fp16.gguf这个模型,对于8700g用核显比用cpu快约25倍

| 项目             | 8700gCPU                   | 780m                       |
| ---------------- | -------------------------- | -------------------------- |
| sampling time    | 44491.53 tokens per second | 55118.11 tokens per second |
| load time        | 265.35 ms                  | 479.18 ms                  |
| prompt eval time | 0.72 tokens per second     | 17.97 tokens per second    |
| eval time        | 53.52 tokens per second    | 53.41 tokens per second    |


+ `load time`: 模型的加载时间
+ `sample time`: prompt做tokenize的时间
+ `prompt eval time`: 预填充(`prefill`)阶段耗时
+ `eval time`: 自回归解码(`decoding`)阶段耗时

### ollama

正常llama.cpp都不会被直接安装使用,毕竟它主要是推理框架,推理的模型我们还得自己维护.ollama就是这个可以管理模型的工具.
作为llama.cpp的上层管理工具,ollama自然是可以顺利执行的.它在设计上充分参考了docker--一样的c/s结构,一样的用systemd管理服务,一样定义了一种打包方式用于专门打包模型,一样的有一个中心化的ollma hub用于上传和分化打包好的模型,对习惯docker的用户来说就相当好上手.

安装只需要挂上代理常规安装即可

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

在rocm装好的情况下这个脚本会安装[ollama-linux-amd64-rocm.tgz](https://github.com/ollama/ollama/releases/tag/v0.5.4)这样的版本.

ollama本质上是一个go程序,正常情况下会被安装到`/usr/local`,同时会配置`systemd`到`/etc/systemd/system/ollama.service`.由于我们是780m,要让igpu成为首选就需要做如下设置:

1. 先停掉`ollama`

    ```bash
    sudo systemctl stop ollama.service
    ```

2. 进入systemd的设置页设置ollama.service的启动环境(一般文件都还没有,需要创建)

    ```bash
    sudo su
    cd /etc/systemd/system/
    mkdir ollama.service.d
    cd ollama.service.d
    nano override.conf
    ```

    填入如下内容

    ```bash
    [Service]
    Environment="HSA_OVERRIDE_GFX_VERSION=11.0.0" # 780m
    Environment="OLLAMA_MAX_LOADED_MODELS=1" # 仅加载一个模型
    Environment="OLLAMA_NUM_PARALLEL=1" # 仅允许一个并发
    ```

    当然了如果有其他要设置的也在这里设置,设置项可以用`ollama serve --help`查看

3. 重新加载ollama.service的设置,并重启

    ```bash
    sudo systemctl daemon-reload
    sudo systemctl restart ollama.service
    ```

但至少截止到2025年01月07日的主干版本为止还存在问题:

当我们要运行一个大模型时,ollama会根据`vram`的大小来判断是否要让cpu参与推理,参与的程度就是

```bash
(模型大小-vram):vram=cpu:gpu
```

要使用某个模型也很简单

```bash
ollama run qwen2:7b-instruct-fp16
```

<!-- 
但很尴尬的是实际在gpu中执行时又都是放在`GTT`中执行的,`vram`一点都不会被用到.详情可以参考这个[issus](https://github.com/ollama/ollama/issues/5471)和这个[pull request](https://github.com/ollama/ollama/pull/6282),作者还提供了一个补丁,只是还没被合进主干.



当然我们也可以根据这个issus修复这个bug

1. 下载源码

    ```bash
    # clone项目
    git clone https://github.com/ollama/ollama.git
    #切到最新的release版本
    git checkout 0.5.7 
    ```

2. 跟着[pull-6282这个pull-request](https://github.com/ollama/ollama/pull/6282/files)修改源码

3. 编译.我是8核16线程的8700g,可以最高16并行度编译,另外也支持和avx512指令集,可以在编译时也带上相关flag以增加性能

    ```bash
    cd 
    make CUSTOM_CPU_FLAGS=avx,avx2,avx512,avx512vbmi,avx512vnni,avx512bf16 -j 8
    ```

4. 部署

    ```bash
    # 先停掉原本的ollama服务
    sudo systemctl stop ollama.service
    # 备份原本的ollama服务
    sudo cp /usr/local/bin/ollama /home/hsz/WorkSpace/rollback/ollama
    # 替换原来的ollama
    cd ~/WorkSpace/Github/ollama/ollama
    sudo cp ollama /usr/local/bin/ollama
    # 重启ollama服务
    sudo systemctl daemon-reload
    sudo systemctl restart ollama.service
    ``` -->

### 交互界面

明显每次要用大模型还得开个terminal是很麻烦,我们可以使用[Jeffser](https://github.com/Jeffser/Alpaca/wiki/Installation)作为交互界面,需要注意比较保守的是下载[3.2.0版本](https://github.com/Jeffser/Alpaca/releases/tag/3.2.0),而且下载无ollama版本.这样我们可以自己给ollama做定制相对更灵活.最新的3.5.0版本我这边是无法打开.

Jeffser只支持linux和macos,而且不支持rag.它仅仅是一个用于"聊天"的交互界面.不过通常情况下是够了的.

### llm使用

上面的3个基础组件其实已经够基础使用了.剩下的就是如何使用.

https://sspai.com/post/94875
https://sspai.com/post/95262
https://sspai.com/post/94482

#### 选择大模型

模型性能来说当然是越大精度越高越好,但我们也是需要考虑使用场景和硬件限制的.

我们部署的是端侧大模型,那不妨看看最先进的苹果是怎么做端侧ai的

##### 苹果的选择

参照苹果基座大模型(AFM)的论文2的架构图,`Apple Intelligence`大致分为两部分:

+ 端侧模型(AFM-on-device),约`2.73B`的模型,经过混合精度的量化,能够在`Apple Neural Engine`上运行
+ 云端模型(AFM-server),未知参数量,运行在`Apple silicon`服务器上,并通过隐私云计算(Private Cloud Compute)提供隐私安全保证.

以这个`2.73B`的模型为例,经过量化之后每个权重大概是`3.5 bits`,那么运行时需要占用的统一内存为`2.73*3.5/8=1.19 GB`.苹果目前最新版本的丐中丐版本配置16g内存.在不影响其他跑cpu的应用使用的前提下,也就是说苹果预留了约15G不到一点的内存给正常程序和其他特定用途大模型使用. 我们家庭自组端侧或边缘llm服务自然也可以利用类似的思路,而且可以放宽端侧模型的大小,毕竟正常pc32g内存已经是主流了.像我这台是96g内存,划16g给核显绰绰有余.

##### 大模型的类型

厂商的模型命名通常没有一个统一的规范,但大致的规律还是有的,我们总结如下

一个典型的大模型命名如下

`qwen2.5:1.5b-instruct-fp16`

+ 模型名和版本号：例如`Qwen-2.5`,`Llama-3.1`,一般来说是版本越高越好,不过也不一定全是提高,比如据说qwen2就比2.5说话更有人味儿
+ 模型参数量: 例如`1.5B`,`7B`,`70B`等等,消费级显卡能跑起来的模型大小一般在`10B`以下,同样也是越大性能越好,但推理速度越慢,显存需求也越大.
+ 是否经过对话对齐：一般带`Chat/Instruct`的模型是能进行对话的,`Base`模型只能做补全任务
+ 特殊能力: `Coder/Code`一般表示经过代码加训,`Math`一般表示经过数学增强等.
+ 多模态: `Vision`或者`VL`代表具有视觉能力,`Audio`能进行音频处理等等
+ 模型的量化格式: `GGUF`为了节省显存进行了量化,一般常见的量化标签有
    + `fp16`,即不做像量化的模型
    + `q{n}_0`,原始的n位对称量化
    + `q{n}_1`,原始的n位非对称量化
    + `q{n}_K[_M|S]`,的n位层次化量化,其中`M/S`并表示应用的位置,大致可以理解为`M`会将`q{n+1}_K`应用于`attention.wv`, `attention.wo`,`feed_forward.w2`,其他用`q{n}_K`;`S`应用`q{n}_K`于全部

    一般n的取值范围是`[2,8]`.通常`q4`及以上才有使用价值,一般推荐`q5_KM`版本或`q4_KM`版本

##### 什么决定模型能力

根据传统的训练`Scaling Laws`模型训练的`loss`直接和训练的计算量相关(包括模型参数大小,训练的数据量),当`loss`降低到一定程度模型会出现[涌现行为](https://zhuanlan.zhihu.com/p/621438653),即在任务中的准确性有突然的提升.

我们可以直观理解为"脑子越大,书读越多,成绩越好",并会在某个时刻突然"开窍"了.以`Qwen`为例,`Qwen-1.5-72B`训练了`3T tokens`;`Qwen-2.5-72B`训练了`18T tokens`,后者效果自然会更好.

`Qwen-32B`一定会显著好于`Qwen-7B`,但是再往上`Qwen-72B`对于特定应用场景的相对提升可能变小了.

我们大致可以根据模型规模预判一个模型的大致能力

+ 小规模(~<5B): 典型如`Qwen2.5-Coder-3B`,可以胜任基本的语言理解,摘要总结,翻译等入门级别任务.
+ 中等规模(~10B): 典型大小`7B,14B`,可以胜任简单的编程任务和逻辑推理任务,复杂的就不行了
+ 中上规模(~30B): 典型如`Qwen-32B-Coder`,已经能接近市面最好的模型的编程能力了,逻辑推理也比中等规模有显著增强,而且Q4量化之后在`16GB`左右,也算是接近消费级设备上限的甜品尺寸.但一样的不要对它有过多期待.
+ 旗舰型(>70B): 基本是各家最好的模型,不过注意有一类特殊的`MoE`模型,在比较性能的时候通常用激活参数,比如`DeepSeek-V2`,激活参数`21B`,但是全部参数是`236B`,然而推理一般是需要全部加载到内存中的,实际上很难跑到消费级设备上.

当然也有很多的benchmark,但往往benchmark是公开的,很容易就可以被所谓"未来数据"影响从而提高得分.所以评估模型的能力最好还是自己下下来用用.

事实上我们依然需要跳出舆论的影响,对大模型的能力有充分客观的认知:

+ 如果结果的准确性无法被轻易验证,那么使用`LLM`就毫无意义.LLM 会产生幻觉(hallucination),这也让它们变得并非绝对可靠.很多时候如果你能够验证LLM的输出是否正确的话,你其实也就没必要用它了,这也是它尴尬的地方.

+ LLM 都是`金鱼脑袋`,也就是说较短的上下文长度限制了它们的发挥.虽然有些模型使用了更大的上下文长度来训练.但是其有效上下文长度通常小的多.实际上,一个LLM一次只能记住相当于一本书里几章的内容,如果是代码的话则是`2000`到`3000`行(因为代码的token密集度更高).当然也可以通过微调或者使用检索增强生成这类的工具来尝试改善,但是只能说收效甚微.

+ 不要指望LLM写代码的能力.往好了说它们的写码能力也只不过是一个读过大量文档的本科生的水平.让llm写点简单组件还行,涉及到细节,业务的就不行了.如果要让他们做这方面工作,你需要替他们思考--将逻辑业务逻辑拆分成小段让llm来编写,你则负责debug和整合.当然这可以替你省不少写八股文的时间,让你可以花更多时间在在对业务做建模等更宏观的事情上,但并不能降低你的心智负担,整体开发时间也不会少多少.而且在编程语言上llm是严重偏科的,毕竟训练语料就偏,相对质量比较高的是`python`,`js`,`sql`,而像`C`,`C++`这样的基本就不太行了.

+ 不要指望llm的逻辑推理能力,包括数学能力.类似代码能力,逻辑推理能力也不是llm的强项,而且可能比写代码还差的多.而且也是任务越小效果越好.所以那些完全依赖llm作判断触发函数执行特定任务的宣传看看就好.当然不是说agent现在还实现不了,其实还是一样的方法,你把任务拆细,把没那么关键的任务交给llm控制执行其实问题不太大,只是你最好还是在他做好后check一下.

那目前哪些任务比较适合给llm干呢?

+ 校对类工作,比如给一篇文章让他提出修改意见,帮忙润色,比如给一段代码让它写注释,提出改进意见.
+ 翻译,其实原理和校对类工作类似.llm的翻译连贯性很不错,但准确性需要自己把下关.
+ 提供创意,你可以和他讨论不断修改,然后再一点点扩充,以丰富这个创意.比如给llm一个大致方向让它发挥写个故事大纲,然后慢慢讨论整个剧本

##### 什么决定模型的推理速度

模型的推理速度受硬件算力水平,显存带宽,预填充参数规模以及量化方式影响.要想准确分析自己的模型的推理速度需要先了解大模型的推理流程

> llm的推理流程

我们都知道当今的语言大模型实质上的架构都基本一致,是仅有解码器的`Transformer+`.如果你在网上搜什么是`Transformers`很可能会搜到[Jay Alammar 这篇著名的博客](https://jalammar.github.io/illustrated-transformer/).对用户来说这些细节能帮助我们更深入了解内部原理但是并不重要.

大模型本质上是一种特殊的程序:输入指令,往后吐字.模型并不能直接处理文字,现在比较通用的做法是使用`tokenizer`把文本映射到词表中的坐标--可以想象成一本字典,在里面的所有的词或字用从0开始的序号标记,模型编码的时候就是从字典里面读这个的标号,解码的时候反之.下图我们简化一下,每个`token`对应一个字:

```text
      我是
      ^ ^
-------------------
|        LLM      |
-------------------
^ ^ ^ ^ ^ ^
你是谁 ? 我是
```

没错,实际上llm的推理就是填字的操作.用户输入prompt:`你是谁`,模型经过一系列计算,在所有可选词中选取可能性最大的`我`进行输出,然后输出的`我`又再作为输入往后推下一个字得到`是`以此类推直到推理出结束符号后停止推理.

注意上面是理想模型,一般情况下我们不会只选取概率最大的`token`进行输出,这样这个程序的输出变成完全确定,并不利于创意类生成,所以需要额外的参数(`temperature`,`topk`等等)来加入一些采样随机性.

我们可以区分实际计算的两个过程:

1. 预填充(`prefill`):即模型在理解一长段的话.模型在计算的同时需要保存一部分后面生成所需要的中间状态(KV Cache),毕竟每次循环都会用到之前的数据,没有必要反复计算
2. 自回归解码(`decoding`):对应迭代生成的过程,模型一步步,token by token的生成结果.

我们在使用流式的大模型应用时,输入请求之后会等一会儿(`prefill`),模型开始出第一个词然后继续输出(`decoding`),也就是对应这两个状态.

而在llm内部,其结构大致可以理解为

```text
    token
      ^
=================== LLM
   unembbeding
-------------------
       ^ ---向量
 transformer blocks
     ....
       ^ ---向量
-------------------
   embbeding
=================== llm使用
       ^
     token
```

基于`Transformers`的语言模型有着非常规整的结构--除了输入输出就是层层叠叠重复的`Transformer blocks`.
每个模型的架构确定,则每一步的计算公式都确定了,例如线性的`f(x)=ax+b`,而模型的权重或者参数即其中的`a`和`b`.
实际上 LLM 的参数量和计算量都要大非常多,以`10B`参数为例,一共有`10^10`个参数,如果都用半精度(FP16/BF16)存储,大小为`10^10 * 16/8 bytes = 20 GB`.

> 机器怎么做推理

上面是算法层面流程,机器又是怎么执行这些算法的呢?我们以我们的核显作为基准,假设模型可以完整加载到显存来看看

1. 把模型参数加载到显存中
2. 小部分计算参数搬运到GPU核心中参与计算并返回

```text
|-----------------------------|
|  模型文件                    |    硬盘            
|-----------------------------|     
    |
 载入(ssd硬盘读速度) 
    |
    v
|-------------------------------------------|
|  模型文件                                  |   Vram/GTT
|    |                                      |
|    v                                      |
|反量化并取出要计算的参数部分(cpu指令速度,GPU规模) |
|-------------------------------------------|
    |             ^
 载入(显存速度)     |
    |        返回结果(显存速度)
    v             |
|---------------------------------|
|     计算(GPU算子速度)             |  iGPU
|---------------------------------|
```

那就可以看出来了,模型推理大致可以分成两段时间

1. 模型加载到显存中的时间
2. 模型参数从显存中取出计算后返回的时间

其中第一段一般可以预加载,即在做推理之前就先将模型加载到显存,因此第一段用户一般感知不到.而第二段就是实打实的用户必然能感知到的推理速度了.

那由这个流程可知,影响到推理速度的因素有

1. 拆分模型参数时的cpu指令速度,这个一般可以忽略不计
2. 显存速度,这个影响计算参数的加载和返回结果的吐出速度
3. GPU规模,影响每次可以计算的规模即吞吐量.
4. GPU算子速度.这个影响计算的速度.

如果显存不够放下全部大模型的参数,在llama.cpp中就会预先拆分一部分给CPU跑.流程和上面GPU的类似,但注意这个过程是串行的不是并行的,也就是说先跑完GPU的部分才会再跑CPU的部分.这就会大大拖慢推理速度.

但无论是全在GPU中跑还是拆分了跑,从llm的推理流程来说其实每个步骤都是在跑llm完整模型,这个时间其实都是一致的.

> 如何提升LLM推理速度?

在讨论这个问题之前我们先定义这里的速度究竟是什么?前面我们提到推理的时候分为两个阶段,对应可以有两类指标评价:

+ 预填充(`prefill`)阶段: 固定上下文长度下,第一个生成词出来的时间(Time to Fist Token, TTFT),单位`ms`;也可以用每秒处理多少个 token 的速度定义(Prompt Processing,PP),单位`tokens/s`
+ 自回归解码(`decoding`)阶段:固定上下文长度,每个输出词的平均耗时(Time per Output Token, TPOP),单位`ms`;也可以用文本生成的速度定义(Text Generage,TG),单位也是`tokens/s`.

在PC游戏和装机领域我们经常会听到CPU瓶颈,显卡瓶颈对游戏性能的影响.同样对于`LLM推理`,在大多数情况下对于GPU推理来说有这样的结论--在预填充阶段主要是算力瓶颈(GPU规模+GPU算子速度),`Transformer`并行处理`prompt token`需要大量的计算;在自回归解码阶段主要是带宽瓶颈,计算不够密集导致参数还没搬过来只能空等.

我们记住几个公式:

```text
预填充时间 = 提示token数 * (参数量 / 计算 TFLOPS)
每秒自回归解码速度 = 参数量 / 带宽
总延迟 = 预填充时间 + 生成token数 * 单token解码时间
```

显然每一个周期的性能也就只能由总延迟来评判.那一个模型的推理速度我们就可以用llama.cpp推理结束后的统计值进行评估了.

对于更加严肃准确一点的推理入门知识可以参考[A guide to LLM inference and performance | Baseten Blog](https://www.baseten.co/blog/llm-transformer-inference-guide/).

对于硬件来说我们可以定义一个`ops/byte`的性能指标,即搬运多少数据时可以做多少计算,来查看是算力不够还是带宽不够.注意在LLM推理中无论模型是否量化,计算都应该在`FP16`或者更高的精度.所以我们在衡量算力的时候应该用FP16的指标,比如`4060Ti`的`FP16`算力是`22.06 TFLOPS`(每秒执行浮点数计算的次数).这个算力除以内存或者显存带宽就可以算出相应指标，比如 4060Ti 的显存带宽是`288GB/s`,即`76.6 ops/byte`。但是对于专用的推理卡比如`A10`,`FP16`算力为`125 TFLOPS`,24GB GDDR6的显存带宽达到`600 GB/s`则能达到`208 ops/byte`.而我们的780m不超频的情况下`FP16`算力是`16.59 TFLOPS`,而显存(即内存)在双通道DDR5 5600下最大为`30GB/s`.该性能指标也就`553 ops/byte`.

当然这个值并不是越高就越说明性能强劲,它反应的是带宽是否成为推理的瓶颈,比如当上下文长度为`4096`时,Llama 2注意力部分的计算密度为`62 ops/byte`,那说明以上例子中的3款显卡显存(内存)带宽都不是瓶颈,那瓶颈不在显存带宽上自然也就在算子速度上了,这个指标越偏离目标计算密度说明带宽颈越大.由此可见`4060Ti`最为均衡考虑到价格还真是推理神卡,而780m显然算力有很大的带宽.我们可以通过超频到3200大致获得10%左右的算力性能提升,而通过超频内存到8400可以让内存带宽达到50GB/s,此时带宽瓶颈依然存在,但显然已经缓解很多了.

当然如果你的模型是向量化过的,那在每个从显存取出参数的循环中都会增加一个反向量化操作,它也会额外拖慢推理速度.

##### 模型选择

显然能选择模型的范围在我们的硬件已经确定的情况下就已经固定了.固然显存是制约我们使用大规模模型的,推理速度也让我们不会选择大规模的模型

`ollama pull 模型名`可以下载对应的大模型,模型默认会被下载到`/usr/share/ollama/.ollama/models`

虽然上面介绍过在linuxv6.10内核中会有对GTT的支持,但很遗憾目前的ubuntu20.04 TLS(截至到2025年1月)使用的内核版本还是6.8.在这个内核版本下我观察到模型确实会被优先加载到GTT中而非vram,但显存容量依然会限制ollama的模型加载.也就是说目前比较好的策略是给vram设置为16G,然后仅运行`7b fp16`以下的大模型.卡着点的模型规格有

+ 7b/fp16
+ 14b/q5_K_M
+ 14b/q6_K
+ 32b/q2_K
+ 32b/q3_K_S

等到年中ubuntu的内核按计划升级到6.11后我们再回头来看看怎么优化.

至于算力来说780m性能也就1650ti水平,推理速度大致和m1芯片的mac差不多.体感上来说2b左右的模型非常快,但7b左右的模型就已经明显会感觉有点小慢了.

这里给出




##### RAG与应用化



##### 定制模型

https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md



<!-- 
## 基于lamikr/rocm_sdk_builder的ai应用



### onnxruntime

https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/install-onnx.html

https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/install-onnx.html#verify-onnx-runtime-installation

### triton

pytorch_triton



## aigc支持

### comfyui

https://github.com/comfyanonymous/comfyui/issues/2810

https://github.com/vosen/ZLUDA/tree/v3

https://www.bilibili.com/video/BV1x2421F78A/?spm_id_from=333.1007.top_right_bar_window_history.content.click&vd_source=c998bf096c14b49524fadf64ec3e75c8

TORCH_BLAS_PREFER_HIPBLASLT=1 ipython

TORCH_BLAS_PREFER_HIPBLASLT=0 && TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1 python main.py --use-pytorch-cross-attention

https://github.com/hartmark/sd-rocm/blob/main/conf/startup-comfyui.sh

### lora训练

## docker支持

amd提供了[一系列docker镜像](https://hub.docker.com/u/rocm),我们需要根据自己的rocm版本(最后一位可以不匹配)以及用到的环境选对应后缀的.

需要注意rocm的docker支持需要宿主机安装好rocm,且镜像rocm和宿主机的匹配

```bash
docker run -it --device=/dev/kfd --device=/dev/dri --group-add video rocm/rocm-terminal:6.2.1
```

比较常用的镜像包括

+ [rocm/onnxruntime](https://hub.docker.com/r/rocm/onnxruntime),onnxruntime环境,可以选onnxruntime版本和pytorch版本,一般用来做推理侧应用
+ [rocm/pytorch](https://hub.docker.com/r/rocm/pytorch/tags),pytorch环境,后缀可以选ubuntu版本,一般用来做训练和开发,也是最常用的镜像
+ [dev-ubuntu-24.04](https://hub.docker.com/r/rocm/dev-ubuntu-24.04)和[ocm/dev-ubuntu-22.04](https://hub.docker.com/r/rocm/dev-ubuntu-22.04),带rocm的ubuntu环境,一般用来做底层框架
 + [rocm/rocm-terminal](https://hub.docker.com/r/rocm/rocm-terminal),rocm的最小镜像,仅包含与宿主机连接rocm的环境,连操作系统都没有.一般也就用来测试.
 

我们以`rocm/pytorch`举例

```bash
docker pull rocm/pytorch:rocm6.2.3_ubuntu22.04_py3.10_pytorch_release_2.3.0_triton_llvm_reg_issue

docker run -it --device /dev/kfd --device /dev/dri --security-opt seccomp=unconfined rocm/pytorch:rocm6.2.3_ubuntu22.04_py3.10_pytorch_release_2.3.0_triton_llvm_reg_issue
```

注意这个镜像极大,有20G以上,对网络有很高的要求

todo -->