---
layout: post
title: "780m上的AI环境搭建"
series:
    aipc_experiment:
        index: 4
date: 2025-02-07
author: "Hsz"
category: experiment
tags:
    - Linux
    - AIPC
    - Rocm
    - AIGC
    - LLM
    - TTS
    - Voice
    - Stable Diffusion
header-img: "img/home-bg-o.jpg"
update: 2025-02-07
---
# 780m上的AI环境搭建

780m这颗核显在笔记本平台基本已经证明了它的实力--默频约等于1050ti,小超约等于1650,极限超可以摸到1060屁股.我们且不考虑性能.先让它可以被机器学习相关工具调用起来.这也算是一窥amd的AI相关生态.

我们的验证平台是8700g,ubuntu 24.04.在前文中已经安装好了驱动和rocm,现在正式开始探索之旅.

## 核显ai生态的基础

我们都知道核显的显存就是内存,一般正常的主板bios最多给你分配16g内存作为核显的显存.当然16g并不算少,但对于很多情况来说也不多.好在[linux在内核版本`6.10`开始允许为核显分配更多的内存作为GTT内存参与核显运算](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?h=v6.10-rc4&id=eb853413d02c8d9b27942429b261a9eef228f005)

这里解释下现代核显的内存模型.对于核显来说,它并没有自己的显存(`vram`),因此只能从物理内存中"划一块"当作显存使用.因此我们的物理内存就有了两块组成`vram`(核显显存)和`ram`(内存).这两块虽然在物理层面是一样的,但使用时井水不犯河水--`vram`是核显专用,`ram`是cpu专用,这两者由于运作机制不同,数据封装等都不相同,因此即便是想也无法直接混用.但很多时候显存并不够用,这时我们就会希望要是能从`ram`(内存)再分点显存用用就好了.这个再分点给显存的部分就是[GTT内存](https://en.wikipedia.org/wiki/Graphics_address_remapping_table).`GTT`一旦被划分出去那就和`vram`一样不再属于`ram`了也就是成了核显专用的了.`GTT`和`vram`在使用时大体上是没有区别的,他们之间的区别主要是

| 区别 | `VRAM`                   | `GTT`                               |
| ---- | ------------------------ | ----------------------------------- |
| 来源 | 由bios设置划分           | 由操作系统划分                      |
| 性能 | 显示需要的内存性能会更好 | 由于无法直达Framebuffer因此会略差些 |

linux内核的这一特性默认会为`vram`和`GTT`一共划分一半的内存,而且这个容量是可以设置的,比如假如我们有64g的内存,我们想划分48g给`GTT`用可以通过编辑`/etc/modprobe.d/ttm.conf`来调整(以4k页为单位对于48G来说就是)

```txt
ttm pages_limit=12582912
ttm page_pool_size=12582912
```

因此我们完全可以在bios中的将`VRAM`设置为`auto`(默认为512m)让核显的`vram`仅用于显示,计算就全靠`GTT`.

这个特性刚出来半年,很多软件并没有很好的适配,但很显然,这种白占大显存的便宜很快就会跟上的.

当然你要说缺陷那自然也是有缺陷的.我们本质上还是在物理内存上划一块给显卡用,和apu最早的愿景--统一内存寻址还是有很大区别.但相比起apple的黄金内存,英伟达的振金显存,这个方案成本太低了.

## rocm的选择

基座运算库有多重要看看老黄赚多少就知道了.cuda早已占据了最好的生态位,这让amd和英特尔的显卡包括核显都很难受.

回到我们的主题,在ubuntu 24.04环境下的780m上我们能用的基座运算库有两种选择

+ 官方的`rocm`
+ 第三方的`lamikr/rocm_sdk_builder`
只有[官方的rocm](https://github.com/ROCm/ROCm).

不过如果打算使用除pytorch/huggingface套件和llama.cp/ollama外的的其他工具,我们就不能装官方rocm和相关工具,而是要借助[lamikr/rocm_sdk_builder](https://github.com/lamikr/rocm_sdk_builder/tree/master)项目,这个我们后面再说.

## 官方的rocm

官方的[rocm](https://rocm.docs.amd.com/en/latest/)并没有官方支持780m这颗核显(官方目前仅支持7900xtx,7900xt).而且需要注意目前rocm在同时存在amd独显和amd核显的情况下会报错误.因此如果你用的是amd独显需要在bios中禁用核显(amd的这个操作真的很神奇,也因此一般推荐au配n卡).

我们的780m核显虽然可以安装rocm但需要有额外设置而且很多周边工具并不支持(比如onnxruntime等),当然正常用是没啥问题的,而且pytorch和llama.cpp(及其生态比如ollama)是可以正常使用的.我们装官方rocm的时候需要关注下pytorch支持到的版本.截致2025年2月7日,pytorch稳定版支持的rocm版本最高为`6.2.4`,我们也就以这个版本为基础介绍

### 安装

可以使用如下步骤安装官方驱动和rocm:

1. 安装安装器

    ```bash
    sudo apt update # 更新软件包的索引或包列表
    sudo apt install "linux-headers-$(uname -r)" "linux-modules-extra-$(uname -r)" # 根据linux内核来安装对应的linux-headers和linux-modules-extra
    sudo apt install python3-setuptools python3-wheel
    sudo usermod -a -G render,video $LOGNAME # 添加当前用户到渲染和视频分组
    wget https://repo.radeon.com/amdgpu-install/6.2.4/ubuntu/noble/amdgpu-install_6.2.60204-1_all.deb # 下载amdgpu安装工具,这里以6.2.60204为例
    sudo apt install ./amdgpu-install_6.2.60204-1_all.deb #安装rocm安装工具
    sudo reboot #重启后生效
    ```

    上面的代码只是例子,我们安装的是rocm 6.2.4的安装器,具体版本可以查看[rocm发布页](https://rocm.docs.amd.com/en/latest/release/versions.html)

    为什么选这个版本呢?因为pytorch目前(2025-01-02)只支持到6.2版本.

2. 根据使用场景安装需要组件

    上面的代码安装了`amdgpu-install`这个工具,它是一个amdgpu的管理工具,可以用于安装和更新AMDGPU的驱动, `rocm`,`rocm 组件`等amdgpu相关的工具.

    在重启后我们重新进入命令行,然后运行`amdgpu-install`来安装所必须得组件

    ```bash
    amdgpu-install --usecase=rocm,graphics,hip
    sudo reboot
    ```

    支持的usecase可以通过命令`sudo amdgpu-install --list-usecase`查看.

    主要的usecase包括

    + `dkms`,仅安装驱动,其他的所有usecase都会安装驱动所以一般不用这个
    + `graphics`,图形界面相关工具,如果你使用ubuntu桌面系统你就得装,不装很多软件会因为显卡报错无法打开(比如各种electron封装)
    + `multimedia`,开源多媒体库相关工具
    + `multimediasdk`,开源多媒体开发,包含`multimedia`
    + `workstation`,工作站相关工具,包含`multimedia`同时包含闭源的OpenGL工具
    + `rocm`,显卡做异构计算工具,包括OpenCL运行时,HIP运行时,机器学习框架,和rocm相关的库和工具
    + `rocmdev`,rocm开发工具,包含`rocm`和相关的调试开发工具
    + `rocmdevtools`,仅包含`rocm`和相关的调试开发工具
    + `amf`,基于amf编解码器(闭源)的多媒体工具
    + `lrt`,rocm的编译器,运行时和设备库等工具
    + `opencl`,异构计算库opencl相关工具,库和运行时
    + `openclsdk`,包含`opencl`,同时包含opencl的相关开发工具和头文件等
    + `hip`,高性能计算库hip的运行时
    + `hiplibsdk`,包含`hip`,同时包含hip开发相关库和工具以及ROCm的数学库
    + `openmpsdk`,并行计算库openmp的运行时和相关库和工具
    + `mllib`,机器学习相关工具和库,包括MIOpen核心和相关库,以及Clang OpenCL
    + `mlsdk`,包含`mllib`,额外附带MIOpen和Clang OpenCL的开发库
    + `asan`,支持ASAN(内存检测工具)的ROCm工具

    正常情况下使用`amdgpu-install --usecase=rocm,graphics` 安装即可

3. 设置系统连接

    也就是设置相关工具的查找位置

    ```bash
    sudo tee --append /etc/ld.so.conf.d/rocm.conf <<EOF
    /opt/rocm/lib
    /opt/rocm/lib64
    EOF

    sudo ldconfig
    ```

4. 使用`update-alternatives`更新配置ROCm二进制文件的路径.

    ```bash
    update-alternatives --list rocm
    ```

5. 设置环境变量

    rocm安装好后会被放在`/opt/rocm-<ver>`目录,我们不妨设置一个环境变量`ROCM_HOME`

    ```bash
    export ROCM_HOME=/opt/rocm-6.2.4
    ```

    + rocm的可执行文件会放在`/opt/rocm-<ver>/bin`目录.
        如果无法使用rocm工具,可以将它的`bin`目录加入到PATH中

        ```bash
        export PATH=$PATH:$ROCM_HOME/bin
        ```

    + rocm的动态链接库会放在`/opt/rocm-<ver>/lib`目录.
        如果要用到这些动态链接库,可以将它临时加入到`LD_LIBRARY_PATH`

        ```bash
        export LD_LIBRARY_PATH=$ROCM_HOME/lib
        ```

    + rocm的模块则会被放在`/opt/rocm-<ver>/lib/rocmmod`目录.

    + 最后,由于我们使用的是核显780m,所以需要额外设置环境变量`HSA_OVERRIDE_GFX_VERSION`

        ```bash
        export HSA_OVERRIDE_GFX_VERSION=11.0.0
        ```

        这个`11.0.0`对应的是8000系apu核显的版本.顺道一提780m的编号`gfx1103`

    这样,我们的`.zshrc`就有如下内容了

    ```bash
    # ======================================================================= rocm
    export ROCM_HOME=/opt/rocm-6.2.4
    export PATH=$PATH:$ROCM_HOME/bin
    export HSA_OVERRIDE_GFX_VERSION=11.0.0
    ```

6. 检查驱动是否正常

    ```bash
    dkms status
    ```

    这个命令会打印出显卡的状态

7. 检查rocm是否正常安装

    ```bash
    rocminfo # 检查rocm状态
    clinfo # 检查opencl状态
    ```

8. 检查包是否安装正常

    ```bash
    apt list --installed
    ```

### 版本更新

更新版本我们需要完全卸载已有的rocm,驱动和rocm安装器

```bash
sudo amdgpu-install --uninstall # 卸载驱动和库
sudo apt purge amdgpu-install # 卸载安装器
sudo apt autoremove # 卸载对应依赖
sudo reboot # 重启后生效
```

之后在下载新版本的安装器重新安装配置一次即可

## rocm_sdk_builder(推荐)

如果想要用全套ai相关工具,我们还是得借助第三方项目[lamikr/rocm_sdk_builder](https://github.com/lamikr/rocm_sdk_builder/tree/master).
[lamikr/rocm_sdk_builder](https://github.com/lamikr/rocm_sdk_builder/tree/master)是一个第三方的rocm方案.它通过给rocm和相关工具源码打补丁的方式让部分相对较新的显卡(核显)可以获得rocm相关工具的原生支持.刚好780m和ubuntu 24.04在它的支持范围内,我们自然也就可以装.当然缺点就是版本相对低些,目前(2025/02/07)只到rocm 6.1.2版本,目前正在慢慢适配6.2版本.相应的,torch,onnxruntime等依赖rocm的库版本也相对更低些,但它可以正常运行sd等常规ai工具.

安装rocm_sdk_builder有2个条件

1. 需要一个干净的系统,不能安装过amd的官方驱动
2. 需要能翻墙的稳定网络环境,依赖项都是在github上的,网络不稳git操作出问题就必须重新下载否则编译无法通过

满足这些条件后我们就可以安装了

```bash
# 我们依然惯例的将rocm_sdk_builder项目源码放在~/workspace/init_source
mkdir -p workspace/init_source # 构造目录
cd workspace/init_source
git clone https://github.com/lamikr/rocm_sdk_builder.git
cd rocm_sdk_builder
# 切到rocm_sdk_builder_612分支
git checkout releases/rocm_sdk_builder_612
# 安装所需依赖
./install_deps.sh
# 将当前用户添加到render用户组并重启
sudo adduser [当前用户名] render
sudo reboot

cd workspace/init_source/rocm_sdk_builder
# 选择编译针对的显卡,可以多选,这里我们为780m选择gfx1103
./babs.sh -c
# 下载依赖到src_projects目录,注意下载好后观察log有没有错误,有的话将对应的项目的目录删除重新执行,否则编译会出错
./babs.sh -i
# 编译项目,编译中间文件会被放在builddir文件夹下大约会持续5~10小时
./babs.sh -b
```

该项目还提供了额外的常用ai应用项目,包括

+ `llama.cpp`
+ `VLLM`
+ `statble-diffusion-webui`

可以通过下面的命令安装

```bash
./babs.sh -b binfo/extra/ai_tools.blist
```

在编译完成后成果会被安装到`/opt/rocm_sdk_612`目录下--可执行文件被放在`/opt/rocm_sdk_612/bin`下(还包含一个python3.11环境);相关库的头文件被放在`/opt/rocm_sdk_612/include`中,如果要使用相关的环境,可以执行`source /opt/rocm_sdk_612/bin/env_rocm.sh`.

而相关的python库会被编译为whl文件放在项目目录下的`packages/whl`目录下(我这里就是`~/workspace/init_source/rocm_sdk_builder/packages/whl`下).

### 版本更新

这个项目目前还是挺活跃的,`releases/rocm_sdk_builder_612`这个分支现在也还一直在更新打补丁,我们可以在项目根目录下通过如下命令更新补丁并重新编译有更新的项目

```bash
# 更新补丁
./babs.sh -up
# 重新编译安装有新补丁的项目
./babs.sh -b
```

## 不同基座下可用的AI工具项目对比

> python包

| 项目                                                      | 官方Rocm 6.2.4                                                                           | rocm_sdk_builder_612                                               |
| --------------------------------------------------------- | ---------------------------------------------------------------------------------------- | ------------------------------------------------------------------ |
| pytorch</br>torchaudio</br>torchvision</br>torch_migraphx | 可以安装[pytorch官方版本](https://download.pytorch.org/whl/rocm6.2.4)                    | `packages/whl`下的对应wheel安装包(v2.4.1)                          |
| triton                                                    | 可以安装[triton官方版本](https://pypi.org/project/triton/),但一些方法会造成gpu掉驱动挂掉 | `packages/whl`下的对应wheel安装包(v3.0.0)                          |
| bitsandbytes                                              | 无法使用                                                                                 | `packages/whl`下的对应wheel安装包(v0.43.2)                         |
| deepspeed                                                 | 无法使用                                                                                 | `packages/whl`下的对应wheel安装包(v0.15.1)                         |
| mpi4py                                                    | 无法调用gpu                                                                              | `packages/whl`下的对应wheel安装包(v4.0.1)                          |
| onnxruntime_training                                      | 无法安装                                                                                 | `packages/whl`下的对应wheel安装包(v1.18.1)                         |
| vllm                                                      | 运行会报错                                                                               | 使用`ai_tools.blist`构造,`packages/whl`下的对应wheel安装包(v0.6.3) |

> 测试过的软件

| 项目          | 官方Rocm 6.2.4             | rocm_sdk_builder_612                                        |
| ------------- | -------------------------- | ----------------------------------------------------------- |
| llama.cpp     | 正常使用                   | 编译`ai_tools.blist`后自带                                  |
| ollama        | 需要额外设置就可可以调用核 | 需要额外设置就可可以调用核显                                |
| ComfyUI       | 运行会报错                 | 使用`packages/whl`下的对应wheel安装包替代原本的依赖可以运行 |
| SDWebUI-forge | 运行会报错                 | 使用`packages/whl`下的对应wheel安装包替代原本的依赖可以运行 |
| lora-scripts  | 运行会报错                 | 使用`packages/whl`下的对应wheel安装包替代原本的依赖可以运行 |
| LLaMA-Factory | 运行会报错                 | 使用`packages/whl`下的对应wheel安装包替代原本的依赖可以运行 |

从对比可以看出至少目前`rocm_sdk_builder`比官方靠谱太多了.

## rocm_sdk_builder基座下的常用ai环境搭建

`rocm_sdk_builder`编译安装完成后会提供一个脚本`/opt/rocm_sdk_612/bin/env_rocm.sh`用于加载对应的设置到环境变量到

我建议将`source /opt/rocm_sdk_612/bin/env_rocm.sh`直接加到用户的`.zshrc`中

```bash
#========================================================================= rocm
source /opt/rocm_sdk_612/bin/env_rocm.sh
```

这样就不用每次手工激活rocm环境了.

在激活该环境后我们会发现`rocm_sdk_builder`提供了`python`和`jupyter`,同时环境中也有我们编译安装的其他软件的命令行工具,比如`llvm`,`clang`,`clinfo`,`rocminfo`,`rocm-smi`什么的.这些就是在rocm_sdk_builder基座下的常用ai环境搭建的基础.

现在消费级PC硬件上的AI开发使用环境可以看作两条线路

1. 依赖python环境的AI训练推理环境,典型的如Comfyui,lora-script
2. 直接调用底层的AI训练,典型的比如llama.cpp

我这里将我用到的都汇总下方便抄作业.

### pytorch开发环境

我们当然可以直接用`rocm_sdk_builder`提供的`python`和`jupyter`环境做为开发环境,但这么多其实并不保险,万一一个不小心把环境搞乱了那就麻烦了,因此我推荐的策略是只直接使用`rocm_sdk_builder`提供的`jupyter`,然后通过配置一个基于`rocm_sdk_builder`提供的`python`构造的虚拟环境专门用来做开发调试.这样如果即便环境乱了也只要删掉重配一份就好.

> 配置一个基于`rocm_sdk_builder`提供的`python`构造的虚拟环境

```bash
# 找个合适的地方放环境
cd <你环境的位置>
# 构造虚拟环境
python -m venv venv 
# 激活虚拟环境
source venv/bin/activate
# 安装`rocm_sdk_builder`提供的各种python包
pip install <rocm_sdk_builder项目位置>/packages/whl/torch-2.4.1-cp311-cp311-linux_x86_64.whl
pip install <rocm_sdk_builder项目位置>/packages/whl/torchaudio-2.4.1+7506e3c-cp311-cp311-linux_x86_64.whl
pip install <rocm_sdk_builder项目位置>/packages/whl/torch_migraphx-0.0.3-cp311-cp311-linux_x86_64.whl  
pip install <rocm_sdk_builder项目位置>/packages/whl/torchvision-0.20.0a0+bea1d4f-cp311-cp311-linux_x86_64.whl
pip install <rocm_sdk_builder项目位置>/packages/whl/triton-3.0.0+git759b4fe3-cp311-cp311-linux_x86_64.whl
pip install <rocm_sdk_builder项目位置>/packages/whl/deepspeed-0.15.1+77f0e5cb-cp311-cp311-linux_x86_64.whl   
pip install <rocm_sdk_builder项目位置>/packages/whl/bitsandbytes-0.43.2.dev0-cp311-cp311-linux_x86_64.whl
pip install <rocm_sdk_builder项目位置>/packages/whl/mpi4py-4.0.1.dev0-cp311-cp311-linux_x86_64.whl
pip install <rocm_sdk_builder项目位置>/packages/whl/onnxruntime_training-1.18.1+cpu-cp311-cp311-linux_x86_64.whl
pip install <rocm_sdk_builder项目位置>/packages/whl/vllm-0.6.3.dev5+g9e9816f6.rocm614-cp311-cp311-linux_x86_64.whl
# 安装其他常用的工具
pip install transformers # 主库
pip install accelerate # 加速模型加载
pip install 'diffusers[torch]' # sd专用库
# 安装jupyter python kernel相关工具
pip install ipykernel # jupyter的python kernel
pip install ipywidget 
```

> 配置jupyter lab的python核心
`rocm_sdk_builder`提供的`jupyter`的python kernel设置保存在`/opt/rocm_sdk_612/share/jupyter/kernels/python3`,我们进入其中修改`kernel.json`即可

```json
{
 "argv": [
  "<你环境的位置>/venv/bin/python", // <-只要改这一行
  "-m",
  "ipykernel_launcher",
  "-f",
  "{connection_file}"
 ],
 "display_name": "Python 3 (ipykernel)",
 "language": "python",
 "metadata": {
  "debugger": true
 }
}需要设置</br>`/etc/systemd/system/ollama.service.d/override.conf`</br>中的`Service.Environment="HSA_OVERRIDE_GFX_VERSION=11.0.0"`</br>
```

当然了`rocm_sdk_builder`提供的`python`作为默认的python也不是一点用也没有,我们可以用它安装`huggingface_hub`来下载模型.

```bash
pip install huggingface_hub
```

也不要忘了给它配置下国内镜像,由于我们在墙内,下模型就太痛苦了,我们可以先换源[hf-mirror](https://hf-mirror.com/),这可以在`.zshrc`中添加如下环境变量

```bash
#========================================================================== huggingface
export HF_ENDPOINT=https://hf-mirror.com # 下载模型的位置
export HF_HUB_CACHE="~/.cache/huggingface/hub" # 制定模型/数据集缓存位置
```

### llama.cpp

在`rocm_sdk_builder`编译`binfo/extra/ai_tools.blist`后我们就可以直接在terminal中调用`llama.cpp`提供的命令行工具了,比如`llama-cli`等.

### ollama

正常llama.cpp都不会被直接安装使用,毕竟它主要是推理框架,推理的模型我们还得自己维护.ollama就是这个可以管理模型的工具.
作为llama.cpp的上层管理工具,ollama自然是可以顺利执行的.它在设计上充分参考了docker--一样的c/s结构,一样的用systemd管理服务,一样定义了一种打包方式用于专门打包模型,一样的有一个中心化的ollma hub用于上传和分化打包好的模型,对习惯docker的用户来说就相当好上手.

ollama最简单的安装方法只需要挂上代理,在有rocm环境的情况下常规安装即可

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

这个安装脚本检测出你有amd显卡后会下载rocm版本的ollama可执行程序,正常情况下它会被安装到`/usr/local`,同时会配置`systemd`到`/etc/systemd/system/ollama.service`.由于我们是780m,要让igpu成为首选就需要做如下设置:

1. 先停掉`ollama`

    ```bash
    sudo systemctl stop ollama.service
    ```

2. 进入systemd的设置页设置ollama.service的启动环境(一般文件都还没有,需要创建)

    ```bash
    sudo su
    cd /etc/systemd/system/
    mkdir ollama.service.d
    cd ollama.service.d
    nano override.conf
    ```

    填入如下内容

    ```bash
    [Service]
    Environment="HSA_OVERRIDE_GFX_VERSION=11.0.0" # 780m需要伪装成gfx1100,因为原版的ollama并不支持gfx1103的780m
    Environment="OLLAMA_MAX_LOADED_MODELS=1" # 仅加载一个模型
    Environment="OLLAMA_NUM_PARALLEL=1" # 仅允许一个并发
    ```

    当然了如果有其他要设置的也在这里设置,设置项可以用`ollama serve --help`查看

3. 重新加载ollama.service的设置,并重启

    ```bash
    sudo systemctl daemon-reload
    sudo systemctl restart ollama.service
    ```

ollama是那种没有gpu用cpu也能跑的软件,但核显还是显著优于cpu的推理速度.以`deepseek-r1:8b`为例,`780m`在`prompt eval rate`上可以达到`13700 tokens/s`,而cpu只能达到`350 tokens/s`,在`eval rate`受限于内存带宽,就都只有`11 tokens/s`左右(cpu可能因为有缓存所以还比核显略微快一点).当然了我的机器是没怎么超内存的,如果好好超一超`14 tokens/s`应该还是可以达到的

#### 官方Rocm 6.2.4下的ollama

顺带一提,在官方rocm环境下,ollama和在rocm_sdk_builder下一样,只要安装好rocm就可以用同样的方式激活780m的核显用作推理.

### ComfyUI

作为目前最流行,生态最丰富的生图工具,ComfyUI现在几乎是ai生图的代名词.
由于rocm_sdk_builder提供了python ai生态下最关键的几个主流库的支持,ComfyUI就天然可以使用了.不过需要注意,Comfyui的插件生态太过庞大,并不一定所有节点都能被支持.

#### 安装

安装也很简单,假设我们要将环境安装到`~/Webapp`目录下,安装过程可以分成如下步骤

1. 克隆Confyui项目

    ```bash
    cd ~/Webapp
    git clone https://github.com/comfyanonymous/ComfyUI.git
    ```

2. 安装专用环境

    ```bash
    cd ComfyUI
    # 构造虚拟环境
    python -m venv venv 
    # 激活虚拟环境
    source venv/bin/activate
    # 安装`rocm_sdk_builder`提供的各种python包
    pip install <rocm_sdk_builder项目位置>/packages/whl/torch-2.4.1-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/torchaudio-2.4.1+7506e3c-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/torch_migraphx-0.0.3-cp311-cp311-linux_x86_64.whl  
    pip install <rocm_sdk_builder项目位置>/packages/whl/torchvision-0.20.0a0+bea1d4f-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/triton-3.0.0+git759b4fe3-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/deepspeed-0.15.1+77f0e5cb-cp311-cp311-linux_x86_64.whl   
    pip install <rocm_sdk_builder项目位置>/packages/whl/bitsandbytes-0.43.2.dev0-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/mpi4py-4.0.1.dev0-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/onnxruntime_training-1.18.1+cpu-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/vllm-0.6.3.dev5+g9e9816f6.rocm614-cp311-cp311-linux_x86_64.whl
    # 安装其他依赖
    pip install -r requirements.txt
    # 解决应用挂着代理时报错的问题
    pip install 'httpx[socks]'
    ```

3. 构造桌面图标(Desktop shortcut),毕竟是桌面linux系统,方便起见我们还是构造一个桌面图标

    1. 在comfyui项目下找一张图取名为`ComfyUI.png`放在根目录下
    2. 在comfyui项目下构造一个启动脚本`launch.sh`放在根目录下

        ```bash
        #!/bin/bash
        source /opt/rocm_sdk_612/bin/env_rocm.sh
        source ./venv/bin/activate
        python main.py --use-pytorch-cross-attention --auto-launch
        ```

    3. 给`launch.sh`脚本赋予执行权限

        ```bash
        chmod +x launch.sh
        ```

    4. 在`~/.local/share/applications`目录下新建一个文件`ComfyUI.desktop`

        ```toml
        [Desktop Entry]
        Version=0.3.14
        Type=Application
        Name=ComfyUI
        Comment=for Stable Diffusion workflow
        Comment[zh_CN]=Stable Diffusion工作流工具
        Path=<ComfyUI的根目录路径>
        Icon=<ComfyUI的根目录路径>/ComfyUI.png
        Exec=<ComfyUI的根目录路径>/./launch.sh
        Terminal=true
        Categories=Graphics;AudioVideo;Application;ConsoleOnly;
        ```

    5. 重启机器后生效

### SDWebUI-forge

同为stable diffusion的生图工具,SDWebUI-forge继承了automatic1111的原版stable diffusion webui的生态同时对性能和资源占用进行了优化并在生态支持上跟上了comfyui的步伐,如果你习惯stable diffusion webui的生图模式不喜欢工作流,SDWebUI-forge依然是一个好选择.
和ComfyUI类似,由于rocm_sdk_builder提供了python ai生态下最关键的几个主流库的支持,SDWebUI-forge就天然可以使用了.不过需要注意,SDWebUI-forge的插件生态太过庞大,并不一定所有插件都能被支持.

#### 安装

假设我们要将环境安装到`~/Webapp`目录下,安装过程可以分成如下步骤

1. 克隆Confyui项目

    ```bash
    cd ~/Webapp
    git clone https://github.com/lllyasviel/stable-diffusion-webui-forge.git
    ```

2. 安装专用环境

    ```bash
    cd stable-diffusion-webui-forge
    # 构造虚拟环境
    python -m venv venv 
    # 激活虚拟环境
    source venv/bin/activate
    # 安装google-perftools优化内存性能
    sudo apt install google-perftools
    # 安装`rocm_sdk_builder`提供的各种python包
    pip install <rocm_sdk_builder项目位置>/packages/whl/torch-2.4.1-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/torchaudio-2.4.1+7506e3c-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/torch_migraphx-0.0.3-cp311-cp311-linux_x86_64.whl  
    pip install <rocm_sdk_builder项目位置>/packages/whl/torchvision-0.20.0a0+bea1d4f-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/triton-3.0.0+git759b4fe3-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/deepspeed-0.15.1+77f0e5cb-cp311-cp311-linux_x86_64.whl   
    pip install <rocm_sdk_builder项目位置>/packages/whl/bitsandbytes-0.43.2.dev0-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/mpi4py-4.0.1.dev0-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/onnxruntime_training-1.18.1+cpu-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/vllm-0.6.3.dev5+g9e9816f6.rocm614-cp311-cp311-linux_x86_64.whl
    # 安装其他依赖
    pip install -r requirements_versions.txt
    # 解决protobuf冲突
    pip install 'protobuf==3.20.2'
    # 解决应用挂着代理时报错的问题
    pip install 'httpx[socks]'
    ```

3. 构造桌面图标(Desktop shortcut),毕竟是桌面linux系统,方便起见我们还是构造一个桌面图标

    1. 在SDWebUI-forge项目下找一张图取名为`icon.png`放在根目录下
    2. 在SDWebUI-forge项目下构造一个启动脚本`launch.sh`放在根目录下

        ```bash
        #!/bin/bash
        source /opt/rocm_sdk_612/bin/env_rocm.sh
        source ./venv/bin/activate
        python launch.py --api
        ```

    3. 给`launch.sh`脚本赋予执行权限

        ```bash
        chmod +x launch.sh
        ```

    4. 在`~/.local/share/applications`目录下新建一个文件`SDWebUI.desktop`

        ```toml
        [Desktop Entry]
        Version=4a30c15
        Type=Application
        Name=SDWebUI
        Comment=Stable Diffusion WebUI
        Comment[zh_CN]=Stable Diffusion网页应用
        Path=/home/hsz/WebApps/stable-diffusion-webui-forge
        Icon=/home/hsz/WebApps/stable-diffusion-webui-forge/icon.png
        Exec=/home/hsz/WebApps/stable-diffusion-webui-forge/./launch.sh
        Terminal=true
        Categories=Graphics;AudioVideo;Application;ConsoleOnly;
        ```

    5. 重启机器后生效

### lora-scripts

B站知名赛博菩萨[秋叶大佬](https://space.bilibili.com/12566101?spm_id_from=333.337.0.0)和[青龙](https://space.bilibili.com/219296?spm_id_from=333.337.0.0)参与stable diffusion的finetune训练项目,底层还是[kohya-ss/sd-scripts](https://github.com/kohya-ss/sd-scripts)但人机交互方面好非常多.基本算是国内搞sd的lora训练的标准工具.

#### 安装

假设我们要将环境安装到`~/Webapp`目录下,安装过程可以分成如下步骤

1. 克隆Confyui项目

    ```bash
    cd ~/Webapp
    git clone https://github.com/Akegarasu/lora-scripts.git
    ```

2. 安装专用环境

    ```bash
    cd lora-scripts
    # 构造虚拟环境
    python -m venv venv 
    # 激活虚拟环境
    source venv/bin/activate
    # 安装`rocm_sdk_builder`提供的各种python包
    pip install <rocm_sdk_builder项目位置>/packages/whl/torch-2.4.1-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/torchaudio-2.4.1+7506e3c-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/torch_migraphx-0.0.3-cp311-cp311-linux_x86_64.whl  
    pip install <rocm_sdk_builder项目位置>/packages/whl/torchvision-0.20.0a0+bea1d4f-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/triton-3.0.0+git759b4fe3-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/deepspeed-0.15.1+77f0e5cb-cp311-cp311-linux_x86_64.whl   
    pip install <rocm_sdk_builder项目位置>/packages/whl/bitsandbytes-0.43.2.dev0-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/mpi4py-4.0.1.dev0-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/onnxruntime_training-1.18.1+cpu-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/vllm-0.6.3.dev5+g9e9816f6.rocm614-cp311-cp311-linux_x86_64.whl
    # 安装其他依赖
    pip install -r requirements.txt
    # 解决应用挂着代理时报错的问题
    pip install 'httpx[socks]'
    ```

3. 构造桌面图标(Desktop shortcut),毕竟是桌面linux系统,方便起见我们还是构造一个桌面图标

    1. 在lora-scripts项目下构造一个启动脚本`launch.sh`放在根目录下

        ```bash
        #!/bin/bash
        source /opt/rocm_sdk_612/bin/env_rocm.sh
        source ./venv/bin/activate

        export HF_HOME=huggingface
        export PYTHONUTF8=1

        python gui.py "$@"
        ```

    2. 给`launch.sh`脚本赋予执行权限

        ```bash
        chmod +x launch.sh
        ```

    3. 在`~/.local/share/applications`目录下新建一个文件`lora-scripts.desktop`

        ```toml
        [Desktop Entry]
        Version=4a30c15
        Type=Application
        Name=SDWebUI
        Comment=Stable Diffusion WebUI
        Comment[zh_CN]=Stable Diffusion网页应用
        Path=/home/hsz/WebApps/stable-diffusion-webui-forge
        Icon=/home/hsz/WebApps/stable-diffusion-webui-forge/icon.png
        Exec=/home/hsz/WebApps/stable-diffusion-webui-forge/./launch.sh
        Terminal=true
        Categories=Development;Application;
        ```

    4. 重启机器后生效

### LLaMA-Factory

通用的llm的finetune工具.

#### 安装

假设我们要将环境安装到`~/Webapp`目录下,安装过程可以分成如下步骤

1. 克隆Confyui项目

    ```bash
    cd ~/Webapp
    git clone https://github.com/hiyouga/LLaMA-Factory.git
    ```

2. 安装专用环境

    ```bash
    cd LLaMA-Factory
    # 构造虚拟环境
    python -m venv venv 
    # 激活虚拟环境
    source venv/bin/activate
    # 安装`rocm_sdk_builder`提供的各种python包
    pip install <rocm_sdk_builder项目位置>/packages/whl/torch-2.4.1-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/torchaudio-2.4.1+7506e3c-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/torch_assets/logo.pngmigraphx-0.0.3-cp311-cp311-linux_x86_64.whl  
    pip install <rocm_sdk_builder项目位置>/packages/whl/torchvision-0.20.0a0+bea1d4f-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/triton-3.0.0+git759b4fe3-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/deepspeed-0.15.1+77f0e5cb-cp311-cp311-linux_x86_64.whl   
    pip install <rocm_sdk_builder项目位置>/packages/whl/bitsandbytes-0.43.2.dev0-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/mpi4py-4.0.1.dev0-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/onnxruntime_training-1.18.1+cpu-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/vllm-0.6.3.dev5+g9e9816f6.rocm614-cp311-cp311-linux_x86_64.whl
    # 安装其他依赖
    pip install -r requirements.txt
    # 解决应用挂着代理时报错的问题
    pip install 'httpx[socks]'
    # 安装其他额外依赖
    pip install --upgrade huggingface_hub
    pip install nltk
    pip install jieba
    pip install rouge-chinese
    pip install transformers_stream_generator
    # 将本项目安装到环境
    pip install --no-deps -e .
    ```

3. 构造桌面图标(Desktop shortcut),毕竟是桌面linux系统,方便起见我们还是构造一个桌面图标

    1. 在lora-scripts项目下构造一个启动脚本`launch.sh`放在根目录下

        ```bash
        #!/bin/bash
        source /opt/rocm_sdk_612/bin/env_rocm.sh
        source ./venv/bin/activate

        llamafactory-cli webui
        ```

    2. 给`launch.sh`脚本赋予执行权限

        ```bash
        chmod +x launch.sh
        ```

    3. 在`~/.local/share/applications`目录下新建一个文件`LLaMA-Factory.desktop`

        ```toml
        [Desktop Entry]
        Version=0.9.2.dev0
        Type=Application
        Name=LLaMA-Factory
        Comment=LLM finetune tools
        Comment[zh_CN]=LLM的微调工具
        Path=/home/hsz/WebApps/LLaMA-Factory
        Icon=/home/hsz/WebApps/LLaMA-Factory/assets/logo.png
        Exec=/home/hsz/WebApps/LLaMA-Factory/./launch.sh
        Terminal=true
        Categories=Development;Application;
        ```

    4. 重启机器后生效

### GPT-SoVITS

全功能的tts工具,

#### 安装

假设我们要将环境安装到`~/Webapp`目录下,安装过程可以分成如下步骤

1. 克隆Confyui项目

    ```bash
    cd ~/Webapp
    git clone https://github.com/RVC-Boss/GPT-SoVITS.git
    ```

    之后为了依赖不冲突,进入项目根目录修改`requirements.txt`,把其中`numba`的版本依赖给去掉

2. 安装专用环境

    ```bash
    cd GPT-SoVITS
    # 构造虚拟环境
    python -m venv venv
    # 安装ffmpeg
    brew install ffmpeg
    # 激活虚拟环境
    source venv/bin/activate
    # 安装`rocm_sdk_builder`提供的各种python包
    pip install <rocm_sdk_builder项目位置>/packages/whl/torch-2.4.1-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/torchaudio-2.4.1+7506e3c-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/torch_migraphx-0.0.3-cp311-cp311-linux_x86_64.whl  
    pip install <rocm_sdk_builder项目位置>/packages/whl/torchvision-0.20.0a0+bea1d4f-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/triton-3.0.0+git759b4fe3-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/deepspeed-0.15.1+77f0e5cb-cp311-cp311-linux_x86_64.whl   
    pip install <rocm_sdk_builder项目位置>/packages/whl/bitsandbytes-0.43.2.dev0-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/mpi4py-4.0.1.dev0-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/onnxruntime_training-1.18.1+cpu-cp311-cp311-linux_x86_64.whl
    pip install <rocm_sdk_builder项目位置>/packages/whl/vllm-0.6.3.dev5+g9e9816f6.rocm614-cp311-cp311-linux_x86_64.whl
    # 安装其他依赖
    pip install -r requirements.txt
    # 解决应用挂着代理时报错的问题
    pip install 'httpx[socks]'
    ```

3. 构造桌面图标(Desktop shortcut),毕竟是桌面linux系统,方便起见我们还是构造一个桌面图标

    1. 在SDWebUI-forge项目下找一张图取名为`icon.png`放在根目录下
    2. 在lora-scripts项目下构造一个启动脚本`launch.sh`放在根目录下

        ```bash
        #!/bin/bash
        source /opt/rocm_sdk_612/bin/env_rocm.sh
        source ./venv/bin/activate
        export https_proxy=http://127.0.0.1:7897 
        export http_proxy=http://127.0.0.1:7897 
        export all_proxy=socks5://127.0.0.1:7897

        python webui.py
        ```

    3. 给`launch.sh`脚本赋予执行权限

        ```bash
        chmod +x launch.sh
        ```

    4. 在`~/.local/share/applications`目录下新建一个文件`GPT-SoVITS.desktop`

        ```toml
        [Desktop Entry]
        Version=d8fc921
        Type=Application
        Name=GPT-SoVITS
        Comment=TTS tools
        Comment[zh_CN]=TTS工具
        Path=/home/hsz/WebApps/GPT-SoVITS
        Icon=/home/hsz/WebApps/GPT-SoVITS/icon.png
        Exec=/home/hsz/WebApps/GPT-SoVITS/./launch.sh
        Terminal=true
        Categories=AudioVideo;Music;Application;ConsoleOnly;
        ```

    5. 重启机器后生效,要使用时双击图标即可

#### 配置依赖的模型

光安装好是无法使用的,我们还必须先配置好模型.

+ 预训练模型[必装],去[lj1995/GPT-SoVITS](https://huggingface.co/lj1995/GPT-SoVITS/tree/main)使用git下载其中的全部文件并放到根目录下的`GPT_SoVITS/pretrained_models`文件夹下.

    ```bash
    cd <GPT-SoVITS项目路径>
    cd GPT_SoVITS
    rm -rf pretrained_models
    git clone https://huggingface.co/lj1995/GPT-SoVITS pretrained_models
    ```

+ G2PW模型[中文tts需要安装],去[这个地址](https://paddlespeech.bj.bcebos.com/Parakeet/released_models/g2p/G2PWModel_1.1.zip)下载文件,解压后文件夹改名为`G2PWModel`并放到`GPT_SoVITS/text`目录下

+ UVR5权重[用于语音和音乐背景音等分离],进入[lj1995/VoiceConversionWebUI/uvr5_weights](https://huggingface.co/lj1995/VoiceConversionWebUI/tree/main/uvr5_weights)路径,将其中的所有文件和文件夹下载到`tools/uvr5/uvr5_weights`目录.

+ ASR(Automatic Speech Recognition,自动语音识别)模型,根据不同的语言将模型下载到`tools/asr/models`目录下
    + 中文去下载FunASR中文模型([Damo ASR Model](https://modelscope.cn/models/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch/files),[Damo VAD Model](https://modelscope.cn/models/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch/files),[Damo Punc Model](https://modelscope.cn/models/iic/punc_ct-transformer_zh-cn-common-vocab272727-pytorch/files)这三个模型).

    ```bash
    cd tools/asr/models
    git clone https://www.modelscope.cn/iic/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-pytorch.git

    git clone https://www.modelscope.cn/iic/speech_fsmn_vad_zh-cn-16k-common-pytorch.git

    git clone https://www.modelscope.cn/iic/punc_ct-transformer_zh-cn-common-vocab272727-pytorch.git
    ```

    + 英语和日语去下载[Faster Whisper Large V3](https://huggingface.co/Systran/faster-whisper-large-v3)
  
    ```bash
    cd tools/asr/models
    git clone https://huggingface.co/Systran/faster-whisper-large-v3
    ```

    + 粤语去下载[FunASR粤语模型](https://modelscope.cn/models/iic/speech_uniasr_asr_2pass-cantonese-chs-16k-common-vocab1468-tensorflow1-online/files).

    ```bash
    cd tools/asr/models
    git clone https://www.modelscope.cn/iic/speech_uniasr_asr_2pass-cantonese-chs-16k-common-vocab1468-tensorflow1-online.git
    ```
<!-- 
### 

### https://github.com/deepbeepmeep/YuEGP.git -->